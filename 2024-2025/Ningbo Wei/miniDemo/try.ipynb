{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "--------get main---------\n",
      "Camera - Wikipedia\n",
      "## Contents\n",
      "# Camera\n",
      "## History\n",
      "### 19th century\n",
      "### 20th century\n",
      "### 21st century\n",
      "## Mechanics\n",
      "### Exposure control\n",
      "#### Aperture\n",
      "#### Shutter\n",
      "#### Light meter\n",
      "### Lens\n",
      "### Viewfinder\n",
      "### Film and sensor\n",
      "### Camera accessories\n",
      "#### Flash\n",
      "#### Other accessories\n",
      "## Primary types\n",
      "### Single-lens reflex (SLR) camera\n",
      "### Large-format camera\n",
      "#### Plate camera\n",
      "### Medium-format camera\n",
      "#### Twin-lens reflex camera\n",
      "### Compact cameras\n",
      "#### Instant camera\n",
      "#### Subminiature camera\n",
      "#### Folding camera\n",
      "#### Box camera\n",
      "### Rangefinder camera\n",
      "### Motion picture cameras\n",
      "#### Professional video camera\n",
      "#### Camcorders\n",
      "### Digital camera\n",
      "#### Camera phone\n",
      "## See also\n",
      "## Footnotes\n",
      "## References\n",
      "## Further reading\n",
      "## External links\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_text_from_url(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/117.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        # 发送带有 User-Agent 的 HTTP 请求 -- 如果没有headers，会被判断为爬虫(Web Crawler)而返回503\n",
    "        response = requests.get(url, headers=headers)\n",
    "        print(response.status_code)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')   # Parse the HTML content using BeautifulSoup\n",
    "    \n",
    "        # Convert to Markdown outline\n",
    "        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])   # only need to focusing the first three level\n",
    "        outline = []\n",
    "        for heading in headings:\n",
    "            level = int(heading.name[1])  # Get the heading level (1 to 6)\n",
    "            text = heading.get_text(strip=True)\n",
    "            outline.append(f\"{'#' * level} {text}\")\n",
    "        markdown_outline = \"\\n\".join(outline)\n",
    "\n",
    "        # Try to extract text from the <main> tag\n",
    "        main_content = soup.find('main')\n",
    "        if main_content:\n",
    "            text_content = main_content.get_text(separator='\\n')\n",
    "            print(\"--------get main---------\")\n",
    "        else:\n",
    "            # Fallback to extracting all text if <main> is not present\n",
    "            text_content = soup.get_text(separator='\\n')\n",
    "        clean_text = '\\n'.join(line.strip() for line in text_content.splitlines() if line.strip())\n",
    "\n",
    "        title = soup.title.string if soup.title else \"No title found\"\n",
    "        \n",
    "        return title, clean_text, markdown_outline\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"----!!!error!!!----\")\n",
    "        return f\"Error fetching URL content: {e}\"\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Camera\"\n",
    "# url = \"https://www.amazon.co.uk/s?k=amazon+camera&crid=2J5OAN2PC30VG&sprefix=amazon+camera%2Caps%2C69&ref=nb_sb_noss_1\"\n",
    "title, clean_text, markdown_outline = fetch_text_from_url(url)  # --------- will store into the data base for history view\n",
    "print(title)  # Print the first 1000 characters for brevity\n",
    "print(markdown_outline)\n",
    "\n",
    "# Write the cleaned text to a file\n",
    "with open('benchMark.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(clean_text)   # future bench marks  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract text info from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webpage is about Amazon.co.uk's selection of cameras. It includes information on cookie preferences and privacy notice. There are 124 results for \"amazon camera\" with options such as Blink Mini and Ring Indoor Camera, both with high customer reviews and discounts.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "from config import api_base, api_key\n",
    "\n",
    "api_base = api_base\n",
    "api_key = api_key\n",
    "deployment_name = \"gpt-35-turbo-16k\"  \n",
    "api_version = \"2023-06-01-preview\"\n",
    "\n",
    "client = AzureOpenAI(azure_endpoint=api_base, api_key=api_key, api_version=api_version)\n",
    "\n",
    "prompt = \"Following text is extracted from a website, including its title, main context, outline. Please help me analysis the following content and return a summery that less than 50 words:\\n ##################extracted text##################\\n\"\n",
    "prompt = prompt + \"#######title#######\\n\" + title[:500]\n",
    "prompt = prompt + \"#######main context#######\\n\" + clean_text[:5000]\n",
    "prompt = prompt + \"#######outline#######\\n\" + markdown_outline[:500]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        model=deployment_name, \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful webpage analysis assistant.\"}, \n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=500\n",
    "    )   \n",
    "\n",
    "#去除回复中的所有\\n以及结尾的空格\n",
    "mapped_value = response.choices[0].message.content.strip().replace(\"\\n\", \"\")\n",
    "print(mapped_value) \n",
    "\n",
    "# to do:\n",
    "# - linked to data beas\n",
    "# - try using javaScript(get info directly from user's web page) -- together with HTML, tag 'main'\n",
    "# - try longchain\n",
    "# - grouping\n",
    "# - frontend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
