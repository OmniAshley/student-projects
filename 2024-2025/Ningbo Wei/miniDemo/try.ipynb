{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon.co.uk : amazon camera\n",
      "Select your cookie preferences\n",
      "We use cookies and similar tools that are necessary to enable you to make purchases, to enhance your shopping experiences and to provide our services, as detailed in our\n",
      "Cookie notice\n",
      ". We also use these cookies to understand how customers use our services (for example, by measuring site visits) so we can make improvements.\n",
      "If you agree, we'll also use cookies to complement your shopping experience across the Amazon stores as described in our\n",
      "Cookie notice\n",
      ". Your choice applies to using first-party and third-party advertising cookies on this service. Cookies store or access standard device information such as a unique identifier. The\n",
      "96 third parties\n",
      "who use cookies on this service do so for their purposes of displaying and measuring personalized ads, generating audience insights, and developing and improving products. Click \"Decline\" to reject, or \"Customise\" to make more detailed advertising choices, or learn more. You can c\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_text_from_url(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/117.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        # 发送带有 User-Agent 的 HTTP 请求 -- 如果没有headers，会被判断为爬虫(Web Crawler)而返回503\n",
    "        response = requests.get(url, headers=headers)\n",
    "        # # Send an HTTP request to the given URL\n",
    "        # response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        \n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract all text content\n",
    "        text_content = soup.get_text(separator='\\n')  # Use newline as separator for better readability\n",
    "        clean_text = '\\n'.join(line.strip() for line in text_content.splitlines() if line.strip())\n",
    "        \n",
    "        return clean_text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching URL content: {e}\"\n",
    "\n",
    "# Example usage\n",
    "# url = \"https://en.wikipedia.org/wiki/Camera\"\n",
    "url = \"https://www.amazon.co.uk/s?k=amazon+camera&adgrpid=59911676550&hvadid=578425084723&hvdev=c&hvlocphy=9046400&hvnetw=g&hvqmt=e&hvrand=7231952894446958386&hvtargid=kwd-490591599792&hydadcr=3954_2155624&tag=googhydr-21&ref=pd_sl_3ytx88w35c_e\"\n",
    "text_content = fetch_text_from_url(url)  # --------- will store into the data base for history view\n",
    "print(text_content[:1000])  # Print the first 1000 characters for brevity\n",
    "\n",
    "# Write the cleaned text to a file\n",
    "with open('benchMark.txt', 'w', encoding='utf-8') as file:\n",
    "    # api that can access web directly can help -- length limitation/ cookie text\n",
    "    # Main element in HTML, only extract info with in tag/element 'main'\n",
    "    file.write(text_content[:50000])   # future bench marks  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract text info from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webpage discusses using cookies on Amazon.co.uk for personalized advertising. It also provides options to customize preferences and links to various departments and products available on the website, specifically featuring Amazon cameras. Two camera options are highlighted, along with their ratings, prices, and features.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "api_base = \"https://ai-rum-swe-a909a5-2.openai.azure.com/\"  \n",
    "api_key = \"fb4f676e065c4e238870e2adc0cd5956\" \n",
    "deployment_name = \"gpt-35-turbo-16k\"  \n",
    "api_version = \"2023-06-01-preview\"\n",
    "\n",
    "client = AzureOpenAI(azure_endpoint=api_base, api_key=api_key, api_version=api_version)\n",
    "\n",
    "prompt = \"Following text is extracted from a website. Please help me analysis the following content and return a summery that less than 50 words:\\n ###########extracted text################\\n\"\n",
    "prompt = prompt+text_content[:5000]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        model=deployment_name, \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful webpage analysis assistant.\"}, \n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=500\n",
    "    )   \n",
    "\n",
    "#去除回复中的所有\\n以及结尾的空格\n",
    "mapped_value = response.choices[0].message.content.strip().replace(\"\\n\", \"\")\n",
    "print(mapped_value) \n",
    "\n",
    "# to do:\n",
    "# - linked to data beas\n",
    "# - try using javaScript(get info directly from user's web page) -- together with HTML, tag 'main'\n",
    "# - try longchain\n",
    "# - grouping\n",
    "# - frontend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
