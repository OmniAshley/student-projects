





!pip install pandas
!pip install datasets





import os
import re
import glob
import shutil
import string
import pathlib


# set the data directory
data_dir = os.path.abspath(os.path.join(os.getcwd(),'..','data'))

# give matplotlib a folder to save its configs
os.environ['MPLCONFIGDIR'] = os.path.join(data_dir,'plt_configs')
import matplotlib.pyplot as plt

#give huggingface a folder to save its stuff in too
#you only need this if you are using a huggingface dataset
os.environ['HF_HOME'] = os.path.join(data_dir,'hf_cache')
import datasets

import PIL
import PIL.Image

import pandas as pd
import numpy as np

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras.layers import TextVectorization
from tensorflow.keras.models import load_model


# setup directories, create them if they do not exist within data folder

# I want my data directory to contain two folders, one for each class I am predicting ("cats" or "dogs")

animals_dir = os.path.join(data_dir,'animals')
if not os.path.exists(animals_dir):
    os.makedirs(animals_dir)
    
cats_dir = os.path.join(data_dir,'animals','cats')
if not os.path.exists(cats_dir):
    os.makedirs(cats_dir)
    
dogs_dir = os.path.join(data_dir,'animals','dogs')
if not os.path.exists(dogs_dir):
    os.makedirs(dogs_dir)





dataset = datasets.load_dataset("cats_vs_dogs")









animals_dataset = dataset['train'].to_pandas()
animals_dataset.head()





len(animals_dataset)





cats_df = animals_dataset[animals_dataset['labels'] == 0]
dogs_df = animals_dataset[animals_dataset['labels'] == 1]





cats_df.head()





cats_df['image'][0]['path']





for i in range(len(cats_df)):
    shutil.copyfile(cats_df['image'][i]['path'], os.path.join(cats_dir,str(i)+'.jpg'))





image_count = len(list(pathlib.Path(cats_dir).glob('*.jpg')))
print(image_count)





cats = list(pathlib.Path(cats_dir).glob('*.jpg'))
print(PIL.Image.open(str(cats[0])).size)
PIL.Image.open(str(cats[0]))





print(PIL.Image.open(str(cats[1])).size)
PIL.Image.open(str(cats[1]))





print(PIL.Image.open(str(cats[-1])).size)
PIL.Image.open(str(cats[-1]))






dogs_df.head()





dogs_df['image'][11741]['path']





for i in range(len(cats_df),len(animals_dataset)):
    shutil.copyfile(dogs_df['image'][i]['path'], os.path.join(dogs_dir,str(i)+'.jpg'))





image_count = len(list(pathlib.Path(dogs_dir).glob('*.jpg')))
print(image_count)





dogs = list(pathlib.Path(dogs_dir).glob('*.jpg'))
print(PIL.Image.open(str(dogs[0])).size)
PIL.Image.open(str(dogs[0]))





print(PIL.Image.open(str(dogs[1])).size)
PIL.Image.open(str(dogs[1]))





print(PIL.Image.open(str(dogs[-1])).size)
PIL.Image.open(str(dogs[-1]))









batch_size = 32 
img_height = 180
img_width = 180

train_ds = tf.keras.utils.image_dataset_from_directory(
  pathlib.Path(animals_dir),
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
  pathlib.Path(animals_dir),
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)





class_names = train_ds.class_names
print(class_names)








plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
    for i in range(2):
        ax = plt.subplot(2, 2, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
plt.show()





AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)





normalization_layer = tf.keras.layers.Rescaling(1./255)


normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
first_image = image_batch[0]
# Notice the pixel values are now in `[0,1]`.
print(np.min(first_image), np.max(first_image))






num_classes = len(class_names)

model = tf.keras.Sequential([
  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])





model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])





model.summary()





epochs=10
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)





acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()





data_augmentation = tf.keras.Sequential(
  [
    layers.RandomFlip("horizontal",
                      input_shape=(img_height,
                                  img_width,
                                  3)),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
  ]
)





plt.figure(figsize=(10, 10))
for images, _ in train_ds.take(1):
  for i in range(2):
    augmented_images = data_augmentation(images)
    ax = plt.subplot(2, 2, i + 1)
    plt.imshow(augmented_images[0].numpy().astype("uint8"))
    plt.axis("off")





model = tf.keras.Sequential([
  data_augmentation,
  layers.Rescaling(1./255),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Dropout(0.2),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes, name="outputs")
])





model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


model.summary()


epochs = 15
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)





acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()











model.save('/content/drive/MyDrive/SavedModels/CatsVsDogsClassifier85.hdf5')





model = load_model('/content/drive/MyDrive/SavedModels/CatsVsDogsClassifier85.hdf5')











sample_dog_url = "https://res.cloudinary.com/sliit-academy/image/upload/v1670419721/SampleDog2_t8wveo.jpg"
sample_dog_path = tf.keras.utils.get_file('SampleDog2_t8wveo', origin=sample_dog_url)

img = tf.keras.utils.load_img(
    sample_dog_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)








sample_dog_url = "https://res.cloudinary.com/sliit-academy/image/upload/v1670423426/SampleDog_u8a66v.jpg"
sample_dog_path = tf.keras.utils.get_file('SampleDog_u8a66v', origin=sample_dog_url)

img = tf.keras.utils.load_img(
    sample_dog_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)








sample_cat_url = "https://res.cloudinary.com/sliit-academy/image/upload/v1670423426/SampleCat_dplrtv.jpg"
sample_cat_path = tf.keras.utils.get_file('SampleCat_dplrtv', origin=sample_cat_url)

img = tf.keras.utils.load_img(
    sample_cat_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)








sample_cat_url = "https://res.cloudinary.com/sliit-academy/image/upload/v1670423426/SampleCat2_jhayxv.jpg"
sample_cat_path = tf.keras.utils.get_file('SampleCat2_jhayxv', origin=sample_cat_url)

img = tf.keras.utils.load_img(
    sample_cat_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)








sample_cat_url = "https://res.cloudinary.com/sliit-academy/image/upload/v1670423426/SampleCat_nkrxqg.jpg"
sample_cat_path = tf.keras.utils.get_file('SampleCat_nkrxqg', origin=sample_cat_url)

img = tf.keras.utils.load_img(
    sample_cat_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)








sample_dog_url = "https://res.cloudinary.com/sliit-academy/image/upload/v1670423827/SampleDog3_rtqlhc.jpg"
sample_dog_path = tf.keras.utils.get_file('SampleDog3_rtqlhc', origin=sample_dog_url)

img = tf.keras.utils.load_img(
    sample_dog_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)








sample_image_url = "https://res.cloudinary.com/sliit-academy/image/upload/v1670436080/SampleImage_tfooxf.jpg"
sample_image_path = tf.keras.utils.get_file('SampleImage_tfooxf', origin=sample_image_url)

img = tf.keras.utils.load_img(
    sample_image_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)








sample_image_url = "https://res.cloudinary.com/sliit-academy/image/upload/v1670436080/Sample_axojms.jpg"
sample_image_path = tf.keras.utils.get_file('Sample_axojms', origin=sample_image_url)

img = tf.keras.utils.load_img(
    sample_image_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)








sample_image_url = "https://res.cloudinary.com/sliit-academy/image/upload/v1670436416/SampleImage2_tbolyq.jpg"
sample_image_path = tf.keras.utils.get_file('SampleImage2_tbolyq', origin=sample_image_url)

img = tf.keras.utils.load_img(
    sample_image_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)








sample_image_url = "https://res.cloudinary.com/sliit-academy/image/upload/v1670436416/SampleCatWoman_pt3lhb.jpg"
sample_image_path = tf.keras.utils.get_file('SampleCatWoman_pt3lhb', origin=sample_image_url)

img = tf.keras.utils.load_img(
    sample_image_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)



