{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pagecontent.txt', 'r') as file:\n",
    "    # Read the content of the file\n",
    "    content = file.read()\n",
    "\n",
    "filtered_content = \"\\n\".join(line for line in content.splitlines() if line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "def get_completion(prompt, model=\"gpt-4\"):\n",
    "    messages = [{\"role\":\"user\",\"content\":prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages\n",
    "    )\n",
    "    return response.choices[0].message['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\n  {\\n    \"Missing_Entities\": \"Large Language Model (LLM) agents; financial analysts; retrieval augmented generation (RAG) pipeline\",\\n    \"Denser_Summary\": \"This extensive and thought-provoking narrative speaks on the topic of Large Language Model (LLM) agents, which are utilized by financial analysts to dissect data, especially about financial performance of companies. Particularly, it emphasizes the combined usage of these LLM agents with retrieval augmented generation (RAG) pipelines to enable analysts to rectify complex questions which could otherwise consume a significant amount of time in data assembly and interpretation.\"\\n  },\\n  {\\n    \"Missing_Entities\": \"Agent core; Memory module; Tools; Planning module\", \\n    \"Denser_Summary\": \"The article unpacks the complex architecture of Large Language Model (LLM) agents, in particular, the agent core which holds the central logic and behavioral elements; the memory modules which store the agents\\' thought process and interactions; the supporting tools which are executable programs the agents use to execute tasks; and the planning modules which aid in complex problem-solving by creating an optimised route of action.\"\\n  },\\n  {\\n    \"Missing_Entities\": \"AutoGPT and BabyAGI projects; Task and question decomposition; Reflection or critic technique\",\\n    \"Denser_Summary\": \"Underlining the complexity of LLM agents, the article introduces readers to projects like AutoGPT and BabyAGI, characterized by the agent\\'s ability to solve intricate problems. A key aspect of an LLM agent, according to the article, is its ability to break down or \\'decompose\\' tasks and questions, in conjunction with the use of a reflection or critic technique, which helps fine-tune the agent\\'s plan of action.\"\\n  },\\n  {\\n    \"Missing_Entities\": \"Swarm of agents; Customized AI author agents; Multi-modal agents\",\\n    \"Denser_Summary\": \"Examining the broad spectrum of potential applications, the article highlights some exemplary use cases of LLM agents. This includes a \\'swarm of agents\\'--seamlessly \\'smart\\' microservices working in unison; \\'customised AI author agents\\'--capable of crafting succinct audience-specific content; and \\'multi-modal agents\\'--able to process different forms of inputs such as text, images and audio files.\"\\n  },\\n  {\\n    \"Missing_Entities\": \"Data curation agents; social graph agents; domain expertise agents\",\\n    \"Denser_Summary\": \"Building on the concept of specialized AI agents, the article showcases a couple of innovative types under active development. Among these are \\'data curation agents\\' which expertly sift through, gather and organise relevant data; \\'social graph agents\\' that capture and manipulate data from social networks and \\'domain expertise agents\\' inculcated with specialised knowledge base, tailor-made to answer specific industry-related questions.\"\\n  }\\n]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = f\"\"\"{filtered_content}\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Article: {context}\n",
    "\n",
    "You will generate increasingly concise, entity-dense summaries of the above Article.\n",
    "\n",
    "Repeat the following 2 steps 5 times.\n",
    "\n",
    "Step 1. Identify 1-3 informative Entities (\"; \" delimited) from the Article which are missing from the previously generated summary.\n",
    "Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.\n",
    "\n",
    "A Missing Entity is:\n",
    "- Relevant: to the main story.\n",
    "- Specific: descriptive yet concise (5 words or fewer).\n",
    "- Novel: not in the previous summary.\n",
    "- Faithful: present in the Article.\n",
    "- Anywhere: located anywhere in the Article.\n",
    "\n",
    "Guidelines:\n",
    "- The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~80 words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
    "- The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "- Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n",
    "\n",
    "Remember, use the exact same number of words for each summary.\n",
    "\n",
    "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_data = json.loads(response)\n",
    "\n",
    "with open(\"output.json\", 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
