## **DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation**

**Publication Date: Mar 15 2023**

- Figure 1. With just a few images (typically 3-5) of a subject (left), *DreamBooth*—our AI-powered photo booth—can generate a myriad of images of the subject in different contexts (right), using the guidance of a text prompt. The results exhibit natural interactions with the environment, as well as novel articulations and variation in lighting conditions, all while maintaining high fidelity to the key visual features of the subject.
- these [text-to-image] models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for “personalization” of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes.
- By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images.
- One of the main advantages of such [text-to-image] models is the strong semantic prior learned from a large collection of image-caption pairs. Such a prior learns, for instance, to bind the word “dog” with various instances of dogs that can appear in different poses and contexts in an image.
- Our goal is to ex- pand the language-vision dictionary of the model such that it binds new words with specific subjects the user wants to generate.
- we propose a tech- nique to represent a given subject with rare token identifiers
- We fine-tune the text-to-image model with the input images and text prompts containing a unique identifier followed by the class name of the subject (e.g., “A [V] dog”). The latter enables the model to use its prior knowledge on the subject class while the class-specific instance is bound with the unique identifier. In order to prevent *language drift* [34, 40] that causes the model to associate the class name (e.g., “dog”) with the specific instance, we propose an *autogenous, class-specific prior preservation loss*, which leverages the semantic prior on the class that is embedded in the model, and encourages it to generate diverse instances of the same class as our subject.
- To evaluate this new task, we also construct a new dataset that contains various subjects captured in different contexts, and propose a new evaluation protocol that measures the subject fidelity and prompt fidelity of the generated results.
- Recent large text-to-image models such as Imagen [61], DALL-E2 [54], Parti [72], CogView2 [17] and Stable Diffusion [58] demonstrated unprecedented semantic generation. These models do not provide fine-grained control over a generated image and use text guidance only. Specifically, it is challenging or impossible to preserve the identity of a subject consistently across synthesized images.
- Our goal is to “implant” a new (*unique identifier*, subject) pair into the diffusion model’s “dictionary” . In order to bypass the overhead of writing detailed image descriptions for a given image set we opt for a simpler approach and label all input images of the subject “a [identifier] [class noun]”, where [identifier] is a unique identifier linked to the sub-ject and [class noun] is a coarse class descriptor of the subject (e.g. cat, dog, watch, etc.).
- We use a class descriptor in the sentence in order to tether the prior of the class to our unique subject and find that using a wrong class descriptor, or no class descriptor increases training time and language drift while decreasing performance.
- **Rare-token Identifiers** We generally find existing English words (e.g. “unique”, “special”) suboptimal since the model has to learn to disentangle them from their original
meaning and to re-entangle them to reference our subject. This motivates the need for an identifier that has a weak prior in both the language model and the diffusion model. ********************[Note:******************** They describe a strategy for getting an token identifier in the paper - I didn’t understand the technicalities.]
- In our experience, the best results for maximum subject fidelity are achieved by fine-tuning all layers of the model.This includes fine-tuning layers that are conditioned on the text embeddings, which gives rise to the problem of *lanuage drift*. Language drift has been an observed problem in language models [34, 40], where a model that is pre-trained on a large text corpus and later fine-tuned for a specific task progressively loses syntactic and semantic
knowledge of the language. To the best of our knowledge, we are the first to find a similar phenomenon affecting diffu-sion models, where to model slowly forgets how to generate
subjects of the same class as the target subject.
- Another problem is the possibility of *reduced output diversity*. Text-to-image diffusion models naturally posses high amounts of output diversity. When fine-tuning on a
small set of images we would like to be able to generate the subject in novel viewpoints, poses and articulations. Yet, there is a risk of reducing the amount of variability in the
output poses and views of the subject (e.g. snapping to the few-shot views). We observe that this is often the case, especially when the model is trained for too long.
- we propose an autogenous class-specific prior preservation loss that en-
courages diversity and counters language drift. In essence, our method is to supervise the model with its *own generated samples*, in order for it to retain the prior once the few-shot fine-tuning begins. This allows it to generate di- verse images of the class prior, as well as retain knowl- edge about the class prior that it can use in conjunction with knowledge about the subject instance.
- we reference the subject’s unique identifier using [V].
- **Evaluation Metrics** One important aspect to evaluate is subject fidelity: the preservation of subject details in generated images. For this, we compute two metrics: CLIP-I and DINO [10].
- The second impor- tant aspect to evaluate is prompt fidelity, measured as the average cosine similarity between prompt and image CLIP embeddings. We denote this as CLIP-T.
- Figure 6. **Encouraging diversity with prior-preservation loss.** Naive fine-tuning can result in overfitting to input image context and subject appearance (e.g. pose). PPL acts as a regularizer that alleviates overfitting and encourages diversity, allowing for more pose variability and appearance diversity.
- We compare our results with Textual Inversion, the re- cent concurrent work of Gal et al. [20], using the hyperpa- rameters provided in their work. We find that this work is the only comparable work in the literature that is subject- driven, text-guided and generates novel images. We gen- erate images for DreamBooth using Imagen, DreamBooth using Stable Diffusion and Textual Inversion using Stable Diffusion. We compute DINO and CLIP-I subject fidelity metrics and the CLIP-T prompt fidelity metric.
- We find that DreamBooth (Imagen) achieves higher scores for both subject and prompt fidelity than DreamBooth (Stable Dif- fusion), approaching the upper-bound of subject fidelity for real images. We believe that this is due to the larger expres- sive power and higher output quality of Imagen.
- Given a prompt “a painting of a [V] [class noun] in the style of [famous painter]” or “a statue of a [V] [class noun] in the style of [famous sculptor]” we are able to generate artistic renditions of our subject.
- Figure 9. **Failure modes.** Given a rare prompted context the model might fail at generating the correct environment (a). It is possible for context and subject appearance to become entangled (b). Finally, it is possible for the model to overfit and generate images similar to the training set, especially if prompts reflect the original environment of the training set (c).
- **Property Modification** We are able to modify subject properties. For example, we show crosses between a specific Chow Chow dog and different animal species in the
bottom row of Figure 8. We prompt the model with sentences of the following structure: “a cross of a [V] dog and a [target species]”.
- Other property modifications are possible, such as material modification (e.g. “a transparent [V] teapot” in Figure 7). Some are harder than others and depend on the prior of the base generation model.
- The first [failure mode] is related to not being able to accurately generate the prompted context. Possible reasons are a weak prior for these contexts, or difficulty in generating both the subject and specified concept together due to low proba- bility of co-occurrence in the training set.
- Other limitations are that some subjects are easier to learn than others (e.g. dogs and cats). Occasionally, with subjects that are rarer, the model is unable to support as many subject variations. Finally, there is also variability in the fidelity of the subject and some generated images might contain hallucinated subject features, depending on the strength of the model prior, and the complexity of the semantic modification.
- **Expression Manipulation** Our method allows for new image generation of the subject with modified expressions that are not seen in the original set of subject images. We
show examples in Figure 14. The range of expressiveness is high, ranging from negative to positive valence emotions and different levels of arousal.
- **Property Modification** We are able to modify subject in- stance properties. For example we can include a color adjective in the prompt sentence “a [color adjective] [V] [class
noun]”. In that way, we can generate novel instances of our subject with different colors.
- **Comic Book Generation…**In Figure 18 we present, to the best of our knowledge, the first instance of a full comic with a persis- tent character generated by a generative model. Each comic frame was generated using a descriptive prompt (e.g “a [V]
cartoon grabbing a fork and a knife saying “time to eat””).

## some notes to self

- **You can use the evaluation protocol proposed in the Dreambooth paper  - it ‘measures the subject fidelity and prompt fidelity of the generated results’**
- Can source dataset images from Unsplash when I’m experimenting
