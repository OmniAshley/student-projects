# papers on tuning diffusion models

## InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning

April 2023

[Visual comparisons on their github project page](https://jshi31.github.io/InstantBooth/)

- On both the “person” and “cat” categories, our model can generate text-aligned, identity-preserved and high-fidelity image variations of the input concept. Vˆ is an identifier representing the input subject. **Our model can instantly generate personalized images with only a single forward pass.**
- However, existing personalization approaches usually require heavy test-time finetuning for each concept, which is time-consuming and difficult to scale. We propose InstantBooth, a novel approach built upon pre-trained text-to-image models that enables instant text-guided image personalization without any testtime finetuning. We achieve this with several major components. First, we learn the general concept of the input images by converting them to a textual token with a learnable image encoder. Second, to keep the fine details of the identity, we learn rich visual feature representation by introducing a few adapter layers to the pre-trained model. We train our components only on text-image pairs without using paired images of the same concept. **Compared to test-time finetuning-based methods like DreamBooth and Textual-Inversion, our model can generate competitive results on unseen concepts concerning language-image alignment, image fidelity, and identity preservation while being 100 times faster.**
- DreamBooth [34] and Textual Inversion [13] generate objects with large pose and location variations of the given concept….However, for each new concept, they need to finetune the pre-trained model for many steps, which is very time-consuming and thus their application scenario is limited. To speed up, **CustomDiffusion [20] and SVDiff [16] reduce the amount of fientuned parameters and** [14] learns an encoder as a good parameter initialization for finetuning. Nevertheless, **these methods still rely on test-time finetuning. In contrast, once trained, our approach does not need any test-time finetuning for each new concept and can instantly generate identity-preserved results, which is a significant improvement on efficiency.**
- Conclusion - We presented an approach that extends existing pretrained text-to-image diffusion models for personalized image generation without test-time finetuning. The core idea is to convert input images to a textual token for general concept learning, and introduce adapter layers to adopt rich image representation for generating fine-grained identity details
- **Limitation and Future Work -**  First, we have to separately train the model for each
category. This limitation can be addressed by training the model with more data of different categories together. **In addition, due to the current design of adapter, it can only accept a single concept to provide the identity details**. ***I’m assuming this means I can’t use it for multiple subjects and also they haven’t released their code on Github.**

## ****Multi-Concept Customization of Text-to-Image Diffusion****

**********************************Publisher: IEEE,**********************************  2023 ([Link](https://ieeexplore.ieee.org/document/10203856))

![Screenshot 2024-01-20 at 10.22.53 AM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/6f72ca84-670c-4a4a-ba35-48571350322d/3969f767-d204-44de-be7d-faf1712830a2/Screenshot_2024-01-20_at_10.22.53_AM.png)

- Figure 1. Given a few images of a new concept, our method augments a pre-trained text-to-image diffusion model, enabling new generations of the concept in unseen contexts. Example concepts include personal objects, animals, e.g., *a pet dog*, and classes not well generated by the model, e.g., *moongate* (a circular gate [76]). Furthermore, we propose a method for composing *multiple* new concepts together, for example, V* dog wearing sunglasses in front of a moongate. We denote personal categories with a new modifier token V* .
- **From Abstract: We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (∼ 6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings.**
- ***compositional fine-tuning* –** the ability to extend beyond tuning for a single, individual concept and compose multiple concepts together, e.g., pet dog in front of moongate (Figure 1)
- **Our method is computationally and memory efficient.** To overcome the above-mentioned challenges, we identify a small subset of model weights, namely the key and value mapping from text to latent features in the cross-attention layers [5, 70]. Fine-tuning these is sufficient to update the model with the new concept.
- **To prevent model forgetting, we use a small set of real images with similar captions as the target images.** We also introduce augmentation during fine-tuning, which leads to faster convergence and improved results. **To inject multiple concepts, our method supports training on both simultaneously or training them separately and then merging.**
- **We build our method on Stable Diffusion [1] and experi- ment on various datasets with as few as four training images.**
- our method only requires storing a small subset of parameters (3% of the model weights) and reduces the fine-tuning time (6 minutes on 2 A100 GPUs, 2 − 4× faster compared to concurrent works).
- Similar to our goals, two concurrent works, **DreamBooth [59] and Textual Inversion [17], adopt transfer learning to text-to-image diffusion models [57, 60] via either fine-tuning all the parameters [59] or introducing and optimizing a word vector [17] for the new concept**. Our work differs in several aspects. First, our work tackles a challenging setting: compositional fine-tuning of *multiple* concepts, where concurrent works struggle. Second, we only fine-tune a subset of cross-attention layer parameters, which significantly reduces the fine-tuning time. We find these design choices lead to better results…
- Our proposed method for model fine-tuning, as shown in Figure 2, only updates a small subset of weights in the cross-attention layers of the model. In addition, we use a regularization set of real images to prevent overfitting on the few training samples of the target concepts.

![Screenshot 2024-01-20 at 10.24.14 AM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/6f72ca84-670c-4a4a-ba35-48571350322d/25fc8d25-b550-4347-94a2-51b677e57550/Screenshot_2024-01-20_at_10.24.14_AM.png)

- **Text encoding.** Given **target concept images, we require a text caption** as well. If there exists a text description, e.g., moongate, we use that as a text caption. **For personalization-related use-case where the target concept is a unique instance of a general category**, e.g., pet dog, we introduce a **new modifier token embedding, i.e., V∗ dog**. During training, V∗ is initialized with a rare occurring token embedding and optimized along with cross-attention parameters. An example text caption used during training is, photo of a V∗ dog.
- **Regularization dataset.** Fine-tuning on the target concept and text caption pair can lead to the issue of language drift [34, 41]. For example, training on “moongate” will lead to the model forgetting the association of “moon” and “gate” with their previously trained visual concepts…Similarly, training on a personalized concept of V∗ tortoise plushy can leak, causing all examples with plushy to produce the specific target images. To prevent this, we select a set of 200 regularization images from the LAION-400M [63] dataset with corresponding captions that have a high similarity with the target text prompt, above threshold 0.85 in CLIP [51] text encoder feature space.
- For fine-tuning with multiple concepts, we combine the training datasets for each individual concept and train them jointly with our method. **To denote the target concepts, we use different modifier tokens, V*** , initialized with different rarely-occurring tokens and optimize them along with cross-attention key and value matrices for each layer…**restricting the weight update to cross-attention key and value parameters leads to significantly better results for composing two concepts compared to methods like DreamBooth, which fine-tune all the weights.**
- **Training details.** We train the models with our method for 250 steps in single-concept and 500 steps in two-concept joint training, on a batch size of 8 and learning rate 8 × 10−5 .
- **Evaluation metrics. We evaluate our method on (1) *Image- alignment*, i.e**., the visual similarity of generated images with the target concept, using similarity in CLIP image feature space [17], (2) ***Text-alignment*** of the generated images with given prompts, using text-image similarity in CLIP feature space [23], and (3) ***KID* [7] on a validation set of 500 real images of a similar concept retrieved from LAION-400M to measure overfitting on the target concep**t (e.g., V∗ dog)and forgetting of existing related concepts (e.g., dog). (4) We also **perform a *human preference* study of our method with baselines.**
- We compare our method with the two concurrent works, *DreamBooth* [59] (third-party implementation [77]) and *Textual Inversion* [17]. **DreamBooth fine-tunes all the parameters in the diffusion model, keeping the text transformer frozen, and uses generated images as the regularization dataset**. **Each target concept is represented by a unique identifier, followed by its category name, i.e., “[V] *category***”, where [V] is a rarely occurring token in the text token space and not optimized during fine-tuning. **Textual inversion optimizes a new V* token for each new concept.**
- **Qualitative evaluation.** We test each fine-tuned model on a set of challenging prompts. This includes generating the target concept in a new scene, in a known art style, composing it with another known object, and changing certain properties of the target concept: e.g., color, shape, or expression. Figure 6 shows the sample generations with our method, DreamBooth, and Textual Inversion. Our method, Custom Diffusion, has higher text-image alignment while capturing the visual details of the target object. It performs better than Textual Inversion and is on par with DreamBooth while having a lower training time and model storage (∼ 5× faster and 75MB vs 3GB storage).
- **Computational requirements** Training time of our method is **∼ 6 minutes (2 A100 GPUs),** compared to 20 minutes for Ours (w/ fine-tune all) (4 A100s), **20 minutes for Textual Inversion (2 A100s), and ∼ 1 hour for DreamBooth (4 A100s)**. Also, **since we update only 75MB of weights, our method has low memory requirements for storing each concept model.**
- We perform the human preference study using Amazon Mechanical Turk.
- **Another way of creating the regularization dataset is to generate images from the pretrained model** [59]. We compare our method with this setup, i.e., for the target concept of a “*category*” generate images using the prompt, photo of a {category}, and show results in Table 3. Using generated images results in a similar performance on the target concept. **However, this shows signs of overfitting**, as measured by KID on a validation set of similar category real images. ***Takeaway for me - don’t build regularisation dataset with images from the pretrained model - find real images online.***
- We show results when no regularization dataset is used. We train the model for half the number of iterations (the same number of target images seen during training). Table 3 shows that the model has slightly lower image-alignment and tends to forget existing concepts… ***Takeaway - must use a regularization dataset.***
- **we augment by randomly resizing the target images during training and append size-related prompts (e.g., “very small”) to the text**. Here, we show the effect of not using these augmentations…Table 3 shows that no augmen- tation leads to lower visual similarity with target images. ***Takeaway - must do data augmentation.***
- **Limitations: difficult compositions, e.g., a pet dog and a pet cat, remain challenging. In this case, the pre- trained model also faces a similar difficulty, and our model inherits these limitations. Additionally, composing increas- ing three or more concepts together is also challenging.**
- More ideas for datasets, images and prompts on their website:  https://www.cs.cmu.edu/~custom-diffusion/ and https://www.cs.cmu.edu/~custom-diffusion/dataset.html
- **Note: [Full version of research paper is on arxiv](https://arxiv.org/abs/2212.04488)  with more details about model architecture, etc.**

## **DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation**

**Publication Date: Mar 15 2023**

- Figure 1. With just a few images (typically 3-5) of a subject (left), *DreamBooth*—our AI-powered photo booth—can generate a myriad of images of the subject in different contexts (right), using the guidance of a text prompt. The results exhibit natural interactions with the environment, as well as novel articulations and variation in lighting conditions, all while maintaining high fidelity to the key visual features of the subject.
- these [text-to-image] models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for “personalization” of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes.
- By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images.
- One of the main advantages of such [text-to-image] models is the strong semantic prior learned from a large collection of image-caption pairs. Such a prior learns, for instance, to bind the word “dog” with various instances of dogs that can appear in different poses and contexts in an image.
- Our goal is to ex- pand the language-vision dictionary of the model such that it binds new words with specific subjects the user wants to generate.
- we propose a tech- nique to represent a given subject with rare token identifiers
- We fine-tune the text-to-image model with the input images and text prompts containing a unique identifier followed by the class name of the subject (e.g., “A [V] dog”). The latter enables the model to use its prior knowledge on the subject class while the class-specific instance is bound with the unique identifier. In order to prevent *language drift* [34, 40] that causes the model to associate the class name (e.g., “dog”) with the specific instance, we propose an *autogenous, class-specific prior preservation loss*, which leverages the semantic prior on the class that is embedded in the model, and encourages it to generate diverse instances of the same class as our subject.
- To evaluate this new task, we also construct a new dataset that contains various subjects captured in different contexts, and propose a new evaluation protocol that measures the subject fidelity and prompt fidelity of the generated results.
- Recent large text-to-image models such as Imagen [61], DALL-E2 [54], Parti [72], CogView2 [17] and Stable Diffusion [58] demonstrated unprecedented semantic generation. These models do not provide fine-grained control over a generated image and use text guidance only. Specifically, it is challenging or impossible to preserve the identity of a subject consistently across synthesized images.
- Our goal is to “implant” a new (*unique identifier*, subject) pair into the diffusion model’s “dictionary” . In order to bypass the overhead of writing detailed image descriptions for a given image set we opt for a simpler approach and label all input images of the subject “a [identifier] [class noun]”, where [identifier] is a unique identifier linked to the sub-ject and [class noun] is a coarse class descriptor of the subject (e.g. cat, dog, watch, etc.).
- We use a class descriptor in the sentence in order to tether the prior of the class to our unique subject and find that using a wrong class descriptor, or no class descriptor increases training time and language drift while decreasing performance.
- **Rare-token Identifiers** We generally find existing English words (e.g. “unique”, “special”) suboptimal since the model has to learn to disentangle them from their original
meaning and to re-entangle them to reference our subject. This motivates the need for an identifier that has a weak prior in both the language model and the diffusion model. ********************[Note:******************** They describe a strategy for getting an token identifier in the paper - I didn’t understand the technicalities.]
- In our experience, the best results for maximum subject fidelity are achieved by fine-tuning all layers of the model.This includes fine-tuning layers that are conditioned on the text embeddings, which gives rise to the problem of *lanuage drift*. Language drift has been an observed problem in language models [34, 40], where a model that is pre-trained on a large text corpus and later fine-tuned for a specific task progressively loses syntactic and semantic
knowledge of the language. To the best of our knowledge, we are the first to find a similar phenomenon affecting diffu-sion models, where to model slowly forgets how to generate
subjects of the same class as the target subject.
- Another problem is the possibility of *reduced output diversity*. Text-to-image diffusion models naturally posses high amounts of output diversity. When fine-tuning on a
small set of images we would like to be able to generate the subject in novel viewpoints, poses and articulations. Yet, there is a risk of reducing the amount of variability in the
output poses and views of the subject (e.g. snapping to the few-shot views). We observe that this is often the case, especially when the model is trained for too long.
- we propose an autogenous class-specific prior preservation loss that en-
courages diversity and counters language drift. In essence, our method is to supervise the model with its *own generated samples*, in order for it to retain the prior once the few-shot fine-tuning begins. This allows it to generate di- verse images of the class prior, as well as retain knowl- edge about the class prior that it can use in conjunction with knowledge about the subject instance.
- we reference the subject’s unique identifier using [V].
- **Evaluation Metrics** One important aspect to evaluate is subject fidelity: the preservation of subject details in generated images. For this, we compute two metrics: CLIP-I and DINO [10].
- The second impor- tant aspect to evaluate is prompt fidelity, measured as the average cosine similarity between prompt and image CLIP embeddings. We denote this as CLIP-T.
- Figure 6. **Encouraging diversity with prior-preservation loss.** Naive fine-tuning can result in overfitting to input image context and subject appearance (e.g. pose). PPL acts as a regularizer that alleviates overfitting and encourages diversity, allowing for more pose variability and appearance diversity.
- We compare our results with Textual Inversion, the re- cent concurrent work of Gal et al. [20], using the hyperpa- rameters provided in their work. We find that this work is the only comparable work in the literature that is subject- driven, text-guided and generates novel images. We gen- erate images for DreamBooth using Imagen, DreamBooth using Stable Diffusion and Textual Inversion using Stable Diffusion. We compute DINO and CLIP-I subject fidelity metrics and the CLIP-T prompt fidelity metric.
- We find that DreamBooth (Imagen) achieves higher scores for both subject and prompt fidelity than DreamBooth (Stable Dif- fusion), approaching the upper-bound of subject fidelity for real images. We believe that this is due to the larger expres- sive power and higher output quality of Imagen.
- Given a prompt “a painting of a [V] [class noun] in the style of [famous painter]” or “a statue of a [V] [class noun] in the style of [famous sculptor]” we are able to generate artistic renditions of our subject.
- Figure 9. **Failure modes.** Given a rare prompted context the model might fail at generating the correct environment (a). It is possible for context and subject appearance to become entangled (b). Finally, it is possible for the model to overfit and generate images similar to the training set, especially if prompts reflect the original environment of the training set (c).
- **Property Modification** We are able to modify subject properties. For example, we show crosses between a specific Chow Chow dog and different animal species in the
bottom row of Figure 8. We prompt the model with sentences of the following structure: “a cross of a [V] dog and a [target species]”.
- Other property modifications are possible, such as material modification (e.g. “a transparent [V] teapot” in Figure 7). Some are harder than others and depend on the prior of the base generation model.
- The first [failure mode] is related to not being able to accurately generate the prompted context. Possible reasons are a weak prior for these contexts, or difficulty in generating both the subject and specified concept together due to low proba- bility of co-occurrence in the training set.
- Other limitations are that some subjects are easier to learn than others (e.g. dogs and cats). Occasionally, with subjects that are rarer, the model is unable to support as many subject variations. Finally, there is also variability in the fidelity of the subject and some generated images might contain hallucinated subject features, depending on the strength of the model prior, and the complexity of the semantic modification.
- **Expression Manipulation** Our method allows for new image generation of the subject with modified expressions that are not seen in the original set of subject images. We
show examples in Figure 14. The range of expressiveness is high, ranging from negative to positive valence emotions and different levels of arousal.
- **Property Modification** We are able to modify subject in- stance properties. For example we can include a color adjective in the prompt sentence “a [color adjective] [V] [class
noun]”. In that way, we can generate novel instances of our subject with different colors.
- **Comic Book Generation…**In Figure 18 we present, to the best of our knowledge, the first instance of a full comic with a persis- tent character generated by a generative model. Each comic frame was generated using a descriptive prompt (e.g “a [V]
cartoon grabbing a fork and a knife saying “time to eat””).
