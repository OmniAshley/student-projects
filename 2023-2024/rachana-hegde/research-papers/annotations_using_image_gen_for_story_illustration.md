# papers on using image generation for illustrating stories/narratives

## Talecrafter: ****Interactive Story Visualization with Multiple Characters****

[Link](https://dl.acm.org/doi/abs/10.1145/3610548.3618184) (IEEE), December 2023

- From Abstract: This paper proposes a system for generic interactive story visualization, capable of handling multiple novel characters and supporting the editing of layout and local structure. It is developed by leveraging the prior knowledge of large language and T2I models, trained on massive corpora. The system comprises four interconnected components: story-to-prompt generation (S2P), text-to-layout generation (T2L), controllable text-to-image generation (C-T2I), and image-to-video animation (I2V). First, the S2P module converts concise story information into detailed prompts required for subsequent stages. Next, T2L generates diverse and reasonable layouts based on the prompts, offering users the ability to adjust and refine the layout to their preferences. The core component, C-T2I, enables the creation of images guided by layouts, sketches, and actor-specific identifiers to maintain consistency and detail across visualizations. Finally, I2V enriches the visualization process by animating the generated images.
- In this work, story visualization is formulated as such a problem, i.e., given a story in plain text and the portrait images of a few characters, generate a series of images to express the story visually. An eligible story visualization should meet several essential requirements to provide an accurate visual representation of a narrative. First, identity consistency. Maintaining consistent depictions of characters and environments across all frames or scenes is crucial. Second, text-visual alignment. The visual content should  align closely with the textual narrative, accurately representing the events and interactions described in the story. Third, clear and logical layout. Objects and characters within the generated images should be arranged in a reasonable and logical layout.
- Our main contributions are in two key aspects:
    - We propose a versatile and generic story visualization system that leverages large language and pre-trained T2I models for generating a video from a story in plain text. This system can handle multiple novel characters and scenes.
    - We develop a controllable, multi-modality text-to-image generation module, C-T2I, which serves as the core component of the visualization system. This module focuses on identity preservation for multiple characters and emphasizes structure control in terms of layout and local structure.
- **Our method targets zero-shot story visualization as well, supporting multiple novel characters and scenes. To ensure identity consistency, we optimize a small set of model weights for each character** and propose a personalized inpainting method to compose multiple characters.
- The basic elements in the description for T2I models are event, scene, and object. Hence, for
a given story, we use the instruction, like **“generate K prompts from the story for Stable Diffusion to generate images, depicting event, character, and scene."** Leveraging the capability of pre-trained T2I models, **we exploit keywords in text to control the style, e.g., using “in oil painting style" as a suffix of the prompts.**
- we collect a test set of ten characters in different styles, each character contains 5-9 images. Based on the resulting model, we train the personalized LoRA weights for each character with the given 5-9 images. **For the story-to-prompt generation, we use the large language model GPT-4.**
- **We compare our method with three approaches: CustomDiffusion [Kumari et al. 2022], Paint-by-Example [Yang et al. 2022], GLIGEN [Li et al. 2023].** **Custom-Diffusion employs a fine-tuning technique for T2I models and enables joint training for multiple
concepts and their composition within a single image.** Paint-byExample constitutes an exemplar-guided image editing method, which facilitates character insertion into images for storytelling purposes. GLIGEN can use a reference image to represent a grounded entity and synthesize images under the guidance of text prompts.
- Fig. 4 shows the story visualization results of an Anime girl and a real cat. **Paint-by-Example, Custom-Diffusion and GLIGEN perform poorly in identity preservation.** They always generate inaccurate clothes and hairstyles. Personalized LoRA weights greatly improve GLIGEN’s ability to maintain identity information. But this is still not enough, it still generates incorrect clothes and hairstyles. However, it destroys the consistency between the image and the prompt, such as the first and fifth columns. Besides, both Paint-by-Example and GLIGEN fail to convert the cat into an Anime cat according to the image style. Custom-Diffusion has a low probability of composing the two characters, it always generates one cat or one girl instead of one cat and one girl, as in the second and third column.
- **Limitations -** Our system builds on the pre-trained Stable Diffusion. **The quality of synthesized images heavily relies on the capability of the pre-trained model.** Since Stable Diffusion (v1.4) performs poorly in face generation, especially when the face covers only a small region in the image, our system inherits this drawback.
- **The components of their pipeline are clearly visible in the website:** https://ailab-cvc.github.io/TaleCrafter/

## **Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models**

Feb 2023

- In summary, the contributions of our work are as follows:
    - We propose **a novel prompt generation pipeline,** in which LLMs understand the context and generate prompt inputs for text-to-image models, replacing the need for human-devised natural language prompts.
    - We propose our **semantic image editing method** and demonstrate its effectiveness against other baselines in terms of smooth editing, **coherency maintenance across independent images, and preservation of background regions.**
- here we introduce a neural pipeline for the **zero-shot generation of coherent story- books without additional training data.** Specifically, our method starts with a simple, yet powerful **prompt generation pipeline that takes as input the plain text of existing stories.** Furthermore, we ensure that the main character maintains a consistent appearance throughout the book by utilizing our proposed **semantic image editing method that injects the de- sired identity into the facial regions**, which play a crucial role in distinguishing a character’s identity.
- Method:
    <img width="658" alt="Screenshot 2024-01-21 at 11 30 37 AM" src="https://github.com/Vis4Sense/student-projects/assets/66835338/0b823977-8a29-4c86-bd01-181617c45ced">

- To further enhance the quality of the generated images, **we employ additional processing of the prompts by the LLM, through the use of magic words…**These magic words, such as ‘highly detailed’ and ‘insanely intricate’ are known to be effective in adding detailed properties to images. **Additionally, when the main subject of the prompt is a person, we also utilize descrip- tions of facial features, such as ‘symmetrical face’ and ‘beautiful eyes’ to enhance the realism of the generated im- ages. This is accomplished through the use of simple in- structions given to the LLM, specifically, ”If the main sub- ject of the prompt is a person, add facial descriptions such as ‘symmetrical face’ or ‘beautiful eyes’**, where the LLM expresses its creativity again.
- In order to generate images in a specific style, we em- ploy the use of style modifiers. In this work, we specif- ically target the style of a storybook.
- Implementation details: For the story-to-prompts compo- nent, we employed the GPT-3 model [4], specifically the ”text-davinci-003” variant, as well as the ChatGPT lan- guage model. For the text-to-image synthesis component, we adopted the Stable Diffusion model(v1.5) [42], a pub- licly available text-to-image latent diffusion model trained on LAION-5B [44] and its default settings. We utilized CodeFormer [59] for face restoration and RetinaFace face detector [9] to accurately detect and align faces in the gen- erated images.

## **Synthesizing Coherent Story With Auto-Regressive Latent Diffusion Models**

2024, [Link](https://openaccess.thecvf.com/content/WACV2024/html/Pan_Synthesizing_Coherent_Story_With_Auto-Regressive_Latent_Diffusion_Models_WACV_2024_paper.html)

- From abstract: In this work, we mainly focus on story visualization and continuation tasks and propose AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images. Moreover, AR-LDM can generalize to new characters through adaptation. To our best knowledge, this is the first work successfully leveraging diffusion models for coherent visual story synthesizing. It also extends the text-conditioned method to multimodal conditioning. Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the adopted challenging dataset VIST containing natural images. Large-scale human evaluations show that AR-LDM has superior performance in terms of quality, relevance, and consistency.
