# Updated Project Plan

The goal of this research project is to build an application for creating a storyboard using images generated by AI to assist writers with brainstorming and outlining their narrative. This will help address writer’s block that can result from lack of inspiration or the struggle to plan out a story or novel sufficiently.

Some of the reasons the images generated by an image generation model such as Dall E are not always useful is that the model will incorrectly render character’s appearances and duplicate or eliminate individuals/objects from the image. This can be an issue if there are several characters who are important to a scene and must be present in the image but are missing or unrecognizable. From the research I have read, it is not possible to fix this problem using prompt engineering with a pretrained LLM unless you further finetune that LLM using techniques such as reinforcement learning and a large dataset containing prompts.

Even after the model generates good images of a character (i.e. images that are aesthetically pleasing and fulfil the writer’s vision), it will not ‘remember’ the character’s appearance. Hence, the character’s appearance will be inconsistent across different settings and scenarios. Addressing this problem and ensuring that the main cast of characters appears the same in each generated image would make it easier for the writer to visualise scenes in their book. This visual consistency will also be useful for storyboarding because it will make the connections between scenes clearer and the writer can identify issues with the narrative arc (i.e. plot holes or incomplete storylines).

Here are the components for this application and how I want to structure the user interaction:

1. The user will enter a one-line summary of their short story or novel. (This can be used later to provide context to GPT-4 when generating plot ideas for the storyboard.)
2. The user will decide who the main cast of 3-5 characters are. They will search for images and/or generate them if they can’t find suitable images.
  a. I can use Pinterest and Unsplash APIs to display image search results to the user so that they don’t have to leave the application. They can select 1 or more of these images.
  b. Or they can generate a brand new image by entering a prompt. Prompt recommendations will be provided by GPT-4 with keyword and style suggestions (and I will use few-shot prompting to instruct GPT for implementing this). For this part, it would be useful for the user to request GPT to help them modify the prompt to emphasize or correct aspects of the image. This has had variable results when I attempted it myself, but the overall result was that the images improved. The user can adjust the prompts until the images they generate are suitable and then save these.
  c. Note – to finetune the model, the dataset needs to have around 3-5 images of the subject, so it is necessary to use prompt engineering to generate similar images of a single subject (or to find multiple images of a person via image search APIs).
3. The user will open the storyboard section of the application which will have boxes where they can enter text or upload/insert images. The user can, then, select up to 3 characters (custom diffusion cannot handle more than three subjects) that they want to appear across multiple scenes and the stable diffusion model will be tuned on these images. This could take up to 10 minutes (this is just an estimate**)**.
4. While the model is being tuned, the user can plan out the narrative arc for their story. The template for the storyboard can follow the traditional three-act structure – inciting incident, rising action, and resolution. This will be framed for the writer as a series of suggestions to guide them.
  a. The user can enter some information for each act and/or scene of their story.
  b. The user can also click a button to generate plot ideas for the next act or scene. This will send instructions to GPT-4 to generate a few ideas based on some context about the story (i.e. the short summary the user entered earlier and any information that they have entered into the storyboard will be sent via prompt chaining). The context length for GPT-4 has increased so it will be possible to send much more information. Additionally, some of the information can also be summarized before it is sent via the API to reduce the number of tokens.
5. The user will be notified that the fine-tuned stable diffusion model is ready for use, and they can copy paste information about the scenes they want images for into the prompt field of the image generation model. The user-entered prompt will be enhanced using GPT-4. (Note: There will have to be some guidance to the user regarding using the modifier token that represents the subject in the prompt.)
  a. The user can generate up to 3 images for each scene and select the one they prefer to add to the storyboard.
6. The user can also move the scenes (a box containing some text and image(s)) around and rearrange them until they are satisfied with the narrative progression.

For the research contribution, I would like to evaluate different methods for finetuning stable diffusion on a small dataset of images of the target subject using techniques such as Dreambooth and Custom Diffusion. [Dreambooth](https://huggingface.co/docs/diffusers/v0.11.0/en/training/dreambooth) enables personalisation of stable diffusion using up to 5 images of the subject. Custom Diffusion, unlike Dreambooth, enables personalising the model for multiple subjects which would be more ideal for my application. The [code for Custom Diffusion is on Github](https://github.com/adobe-research/custom-diffusion) and [there’s a model training guide on Hugging Face](https://huggingface.co/docs/diffusers/training/custom_diffusion) with instructions for modifying the script.

I intend to follow the process described in the research paper for Custom Diffusion and also Dreambooth – i.e. put together the datasets, run and modify code for my specific use case and determine how to improve the output by adjusting parameters. There have been some articles on this but there is no single comprehensive guide and since I would have to make changes to the code to achieve my aim, this could be a research contribution.

I also noticed that the prompts used to test out these finetuning methods in the research papers are simplistic – along the lines of “V* cat sitting on chair” and my assumption is that the pretrained model is quite familiar with images of cats, so it is not as difficult for it to generate images of the same cat in multiple settings. However, I want the user to be able to depict more complex scenes from their story to visualise them better so this would involve using prompts that are at least 10 or more words long. Therefore, I think it would be interesting to see how these methods fare when combined with more complicated prompts and whether the longer prompt can be optimised in some way using GPT-4 to get better results.

My goal is also to develop an application with an interface that will make this personalised model more accessible for a user with less technical expertise and familiarity with tuning an image generation model but wants to generate art for their story.

Research contributions summarized:
- The application will have an interface for user to connect scenes (containing text and images) together to make the storyboard and assist them in outlining. The interface will also optimise the workflow for a writer who wants to design and create a character then train an image generation model on a dataset of character to produce more relevant images for constructing a storyboard.
- The image generation will also provide visual inspiration for character design.
- The combination of plot ideas (from LLM) and more relevant image generation will collectively help address writer’s block.
- I can address a limitation of diffusion model (inconsistent art) using available methods (i.e. Custom Diffusion) for customising diffusion models. I can also evaluate the extent to which these customisation methods will work for my specific use case and how to modify the code provided in past research papers to enhance the results.
