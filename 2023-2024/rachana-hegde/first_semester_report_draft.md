# First Semester Report

## Abstract
TBD

## Index Terms

TBD

## I. Introduction 
**(1100 words)**

Generative  AI is altering conventional approaches to working in multiple industries by creating a paradigm shift through automation and enhanced productivity. Applications for generative AI include email writing, teaching assistance, graphic design, code synthesis, facilitation of drug exploration, copywriting, virtual assistance and content creation [1]. Large language models (LLMs), in particular, such as GPT-3, generate text and carry out tasks such as writing code and autocompletion, often without requiring specialised training [2]. LLMs are also useful for natural language processing tasks such as script and plot generation [2], [3], [4]. Generative image models such as Stable Diffusion [5] are trained to denoise images with guidance from prompts, thereby producing new images [6]. Text-to-image generative models have varied use cases from facilitating industrial design [7] to news illustration [8] and producing images for radiology [9].

This research project explores using generative AI to assist creative writers with brainstorming. The following paragraphs discuss the creative needs of writers, challenges of applying diffusion models to writing tasks and an proposed solution, and the potential contributions of this research project. ****

Writers often utilize Pinterest to find visual inspiration, curating vision boards that evoke the aesthetics and themes of their story. However, it is difficult to find images that precisely capture their vision. This problem is twofold because firstly, it is challenging to find artwork for characters that are fantastical creatures with unconventional appearances. Secondly, even if the character possesses a more generic appearance, it is unlikely that there will be images depicting that character in a specific setting from the story. Traditional solutions for this involve commissioning custom artwork, which is expensive, and searching for images on various websites, which is time-consuming and tedious.

This project addresses this problem by designing an image generation application that writers can use to generate reference images for characters. Some of the reasons that the images generated by an image generation model such as Dall E are not always useful is that the model will incorrectly render character’s appearances and duplicate or eliminate individuals/objects from the image. This can be an issue if there are several characters who are important to a scene and must be present in the image but are missing or unrecognizable.

Even after the model generates aesthetically pleasing images that fulfil the writer’s vision, it will not ‘remember’ the character’s appearance. Hence, the character’s appearance will be inconsistent across different settings and scenarios. Addressing this problem and ensuring that the main cast of characters appears the same in each generated image would make it easier for the writer to visualise scenes in their book. This visual consistency will also be useful for storyboarding because it will make the connections between scenes clearer and the writer can identify issues with the narrative arc (i.e. plot holes or incomplete storylines).

Writers often struggle to decide which ideas to pursue but visualising plot threads as sequence of images could aid in making narrative decisions. This, in turn, will enable this project to address the core research requirement of assisting writers in overcoming writer’s block that can result from lack of inspiration or the struggle to plan out a story sufficiently. Hence, the goal of this research project is to build an application for creating a storyboard using images generated by AI to assist writers with outlining their narrative.

This research project seeks to improve the visual consistency of images produced for storyboarding by optimising prompts with a pretrained LLM (GPT-4) for a diffusion model finetuned on a small dataset of character art. The project will explore the extent to which applying these existing methods of prompting and finetuning can address the limitations of image generation models described above.

One of the motivations for this project is that there are currently no comprehensive and easy to follow/implement guides available to individuals who want to use and customise image generation models. While there are forums such as Reddit Stable Diffusion where anonymous users can discuss the results of experimenting with machine learning models, these forum posts can often be convoluted and difficult to make sense of without the requisite knowledge of machine learning. Implementing the advice found in Medium articles and Twitter posts can also be time consuming and mentally exhausting as this process can involve a great deal of trial and error, demanding hours of debugging error messages full of machine learning jargon.

In summary, this research project aims to make the following contributions:

1. The application will have an interface for user to connect scenes (containing text and images) together to make the storyboard and assist them in outlining.
2. The application will also support writers in finetuning a diffusion model (assuming they have limited knowledge on generative AI). It will facilitate the process for designing characters, producing dataset(s) of character art and training the image generation model on the dataset(s) to produce more relevant images for constructing the storyboard. It will do so by optimising prompts for the diffusion model and facilitating the image generation process to reduce the mental burden on the writer (i.e. by providing advice and guidance at step of the process).
3. The image generation will also provide visual inspiration for character design.
4. The combination of plot ideas (from LLM) and more relevant image generation will collectively help address writer’s block.
5. The project will explore limitations of diffusion models and how these can be circumvented using methods for customising diffusion models. The prototype evaluation will demonstrate the extent to which these customisation methods work for this specific use case of assisting writers with storyboarding and detail how code provided by past researchers was modified to enhance the results from finetuned models.

The following section will explore relevant literature related to prompt optimisation tools, using image generation for story visualisation, writer’s block, using LLMs for creative writing and finetuning diffusion models. Section III will, then, describe how the scope of this research project was refined through the thematic analysis of initial user interviews and the resulting user requirements for the application. Section IV further explores the application design and justifications for utilising the chosen LLM and image generation models. Finally, Section V. will conclude with an overview of future work for this project including building, testing, and evaluating the prototype and the updated project timeline.

## II. Literature Review
**2051 words**

This literature review highlights the origins and mitigation of writer's block in section II-A alongside exploring generative AI potential for aiding in overcoming writer's block. Section II-B is, then, concerned with recent research on incorporating AI into storyboarding software and section II-C covers prompt engineering for generative image models. Section II-D establishes some recent advancements in story visualisation and section II-E investigates the integration of LLMs into writing software. Finally, Section II-F surveys finetuning methods and their limitations. This literature review informs key decisions for the application design.

**Writer’s block and creative needs**

First, it is important to understand the origins of writer’s block and how it can be mitigated. Writer’s block occurs when a skilled writer is unable to generate fresh work for a set amount of time [10]. Contributing factors to writer’s block include stress, depression, diminished motivation, perfectionism, procrastination [11]. According to Ahmed and Güss, writers struggle at various stages in the writing journey such as selecting ideas to develop further, brainstorming, and expressing ideas [11]. By building image generation and storyboarding software, this project seeks to counteract problems such as ‘overplanning’ which Ahmed and Güss describe as deciding a certain outcome for a creative endeavour too early without sufficiently appraising other possibilities [11]. This obsession with a particular solution combined with inflexible thinking can obstruct creative thought processes [11]. This project hypothesizes that accessing a tool that actively encourages writers to view multiple plotlines and branching narratives will foster a more adaptable mindset early on.

To determine how to design an application that will address writer’s block, it is necessary to explore some literature that discusses this phenomenon in relation to generative AI. Namely, Gero et al. conducted interviews with creative authors and shared their findings on how AI writing tools can best support them [12]. Gero et al. note that it would be helpful to manage the writer’s expectations regarding the AI system’s abilities by providing guidance in the form of context, examples of how the AI system will perform for various tasks, and the interface design which separates AI generated text from story inputted by the user [12].

When designing an application for writers, it is also important to recognise the types of support that would be helpful. This research project will target the planning stage of writing which is highly conceptual, involving establishing objectives and developing ideas [12]. For planning, Gero et al. record that writers expressed the desire for inspiration and assistance with writer’s block – particularly when they had difficulty making narrative decisions, they would like to choose from multiple scenarios and/or adapt these computer-generated ideas [12]. An earlier paper also highlights that writers require assistance with establishing plot, ensuring narrative consistency, and creating compelling story arcs [13].

Kreminski and Martens further explain that the writer’s goal is to communicate a cognitive framework for the narrative’s world which contains a cast of character, objects, settings and relationships between these elements which evolve over time [13]. When the narrative grows increasingly complex, it becomes imperative for writers to record details to ensure that there are no contradictions, discrepancies, or plot holes in the story’s chronology [13].  To achieve narrative consistency, writers require storyboards to examine the narrative at a conceptual level.

When considering how to design the storyboard functionality for the application in this research project, Kreminski and Martens’ discussion of models for plotting including the Hero’s Journey and Freytag pyramid are useful to consider to design the template [13]. In fact, Kreminski and Martens break down the plot into the following scenes: the inciting incident that motivates the characters, the character’s journey in trying to address this event, the climax and resolution or aftermath [13].

Thus, this literature will help inform design decisions for the application design.

1. **AI Storyboarding software**

There is less recent research on integrating storyboarding software with AI tools specifically for writing. However, Jo et al. built Gennie which takes user-entered text as input and outputs sketches of scenes from the story [14]. Unlike Gennie which produces ‘draft sketches’ with less detail [14], this project’s objective is to produce higher quality images with greater fidelity to text prompts. More recently, Lee explored using ChatGPT and Midjourney to enhance results from using AiSAC or ‘AI Analysis System for Ad Creation’ which is an AI powered storyboarding tool [15].

### **C. Prompt recommendation tools**

A crucial first step in using generative image models, such as diffusion models, is providing effective prompts. According to Jiang et. al [16], some challenges of prompt engineering include finding examples for prompts, troubleshooting and assessing if prompts are improving. To enhance prompt engineering for non-technical users, research has investigated user-friendly approaches for crafting effective prompts for generative image models. For example, Promptify provides prompt recommendations for text to image models using the LLM GPT-3.5 to suggest keywords and facilitating the organisation of generated images [17]. In contrast, PromptMagician utilises the DiffusionDB dataset’s collection of prompts and images to produce keyword recommendations and filters resulting images [6]. Similarly, RePrompt automatically updates prompts to improve the accuracy of emotions conveyed in images [18].

This project implicitly addresses challenges that users, who are unfamiliar with machine learning models, encounter when attempting to articulate their preferences and requirements to generative AI using prompts. To accomplish this, the research will implement recommendations provided in these papers, including using advanced models such as GPT-4 for prompting, adopting chain of thought prompting for prompt clarification, introducing negative prompts, and the enhancement of user control over multiple hyper-parameters [6], [17].

### **D. Sequential image generation for illustrating stories**

Multiple research papers discuss various methods for story visualisation using a combination of LLMs and image generation models [19], [20], [21]. Namely, Kumagai et al. focus on converting stories into logical visual sequences by addressing problems such as contextual accuracy and consistency [22]. Kumagai et al.’s study [22] uses an LLM to add information about context and characters to each prompt. They experimented with movie scripts to test their method, discovering that it enhances the coherency and contextual authenticity of outputted images. While Kumagai  et al.’s approach is promising, it involved utilising an AR-LDM, an autoregressive latent diffusion model [22]. AR-LDM can convey the connections between sequential images by accounting for captions and previously generated images but it can be confused by unfamiliar character names and pictures might not adhere to the narrative’s flow [22]. Kumagai  et al. used AR-LDM to train a model on ‘character-specific tokens’ to enable the model to obtain information regarding the subject’s physical characteristics from inputted prompts [22]. Although this project will not use AR-LDM, Kumagai  et al.'s research method of modifying prompts by adding contextual factors such as physical qualities of characters to increase consistency of character appearance in different photos is relevant to this project [22]. Hence, the prompt design strategies in this paper [22] were used as inspiration for the methodology.

Previous research has also investigated the use of generative image models to illustrate fairy tales. For instance, Ruskov discusses using Midjourney v4 to create illustrations for fairytales and devising an prompt refinement process for image creation [23]. Another example is FairyTailor, a platform for blending generated text with images from Unsplash to construct children’s fairytales and offers features such as auto-completion [24].

### **E. LLMs for creative writing**

Multiple papers have discussed building writing tools powered by AI and how these can automate parts of the writing process. For example, Dramatron can produce scripts and screenplays by using prompt chaining to create structural context [3]. The method explored in this paper [3], termed ‘hierarchical story generation’, can provide inspiration for working with an LLM during this research project. The paper describes how a log line, which condenses the narrative into a few sentences with information about the setting, protagonist, and conflict, is used to generate the story [3]. In order to build the plot ideas generation feature, it will be necessary to use few shot learning and chain of thought prompting to make OpenAI API calls. This research project will also use one sentence plot summaries as narrative context to guide plot idea generation from the LLM. While the precise prompts from the Dramatron paper [3] will not be applicable, understanding how they generated a cohesive script through prompt programming will be valuable for determining how to provide more relevant plot suggestions to users of the application developed during this research project. However, one of the limitations that was flagged in [3] is that LLMs’ context window is restricted to 2048 tokens in advanced models and this is no longer applicable. OpenAI’s GPT-4-turbo models, for instance, have context windows of up to 128,000 tokens [25] and as this context window continues to increase in the future – it will enable LLMs to produce more accurate plot suggestions based on larger amounts of information about the story.

Two papers that explore using LLMs to augment the writing process, [26] and [2], develop applications with writing interfaces that integrate writing suggestions generated by AI. Wordcraft, for example, employs a Language Model (LLM) in conjunction with an editor and an interface that prompts users to extend the narrative, propose rewrites, and request additional details [2]. Wordcraft can be used for tasks such as generating ideas, rewriting scenes and copyediting [2]. Singh et al., similarly, designed an interface that also provides images from Unsplash and audio from Freesound to further enhance the writing process [26].

Another program, End-to-End Story Plot Generator, can produce a plot using LLaMA2 which removes the rate limit associated with OpenAI API calls and allows for large-scale text generation [4]. Since this paper [4] involves finetuning multiple LLMs and building datasets, for training the models, to improve the quality of the plots generated – it wouldn’t be feasible to implement their entire solution. However, the information in this paper [4] about designing and structuring effective prompts to generate details about premise and characters using an LLM is useful. For example, they note that LLaMA2 emphasised the character’s appearance, so they modified the prompt to ensure that the LLM’s output contained information about the character’s profession and connections to other characters [4].

Other research has explored enabling writers to influence story outline generation by drawing lines that reflect the protagonist’s fate [27] and Yuan et al. incorporate a chatbot which writers can use for generating ideas or conducting research [2].  Furthermore, there are also publicly available AI writing tools for use cases such as copywriting and marketing [28]. To that end, a fiction writing tool called Sudowrite uses Transformer models such as GPT-3 by OpenAI and the diffusion model DALL·E to generate images, text autocompletions, writing feedback, and plot ideas [29], [30].

### **F. Personalising diffusion models**

This section will delve into methods for finetuning diffusion models. Models such as Stable Diffusion [5] can be customised using techniques such as Dreambooth [31],  LoRA [32], Custom Diffusion [33], and Textual Inversion [34]. Dreambooth addresses the inability of pre-trained text-to-image models to replicate subjects from a dataset and create new representations of them in various situations [31]. With Dreambooth, the model requires 3-5 images of a subject to produce images of the subject in new settings by learning to link an distinctive identifier with a particular subject [31]. Some scenarios in which Dreambooth fails include: unfamiliar environments described in prompts, overfitting where photos identical to the training set are produced, rare subjects, and hallucinations [31].

One criticism of the LoRA, Dreambooth, and Textual Inversion fine tuning methods is that they struggle to recreate more than one subject [33]. Custom Diffusion seeks to overcome this constraint and Kumari et al. assert that tuning a few parameters in the ‘text-to-image conditioning mechanism’ allows the model to generate images of multiple subjects [33]. This training can involve merging multiple tuned models through ‘closed-form constrained optimization’ or training for several entities simultaneously [33]. Benefits of using Custom Diffusion include the 6 minute training time which is much shorter than Dreambooth and that it consumes less resources such as memory and model storage space [33]. Despite its benefits,  Custom Diffusion still has difficulty producing images with three or more specific subjects [33].  This research project aims to develop a better understanding of limitations associated with methods such as Dreambooth and Custom Diffusion and potentially discover ways to circumvent them by leveraging recent modifications in the training scripts.

## III. User Requirements
**3700 words including tables**

This section discusses the process of generating user requirements based on a thematic analysis of semi-formal user interviews with three writers in their twenties. Participants were selected on the basis of their interest in exploring generative AI applications in creative writing. Each interview ranged from 30 to 45 minutes and was conducted remotely over Microsoft Teams or Zoom. The video recordings of the interviews were transcribed manually, followed by a thorough analysis to extract key themes.

1. Formative Interviews
2. Interview Design

This subsection summarizes the structure of the interviews. First, the participant was informed about the objective of the research project – to build an image generation application to assist writers – and some details about the motivations for working on this project. The initial list of questions is specified in Table I, but the interview was flexible, so if a participant made a relevant statement, I would offer a follow-up question to better grasp their point of view and keep the conversation flowing.

TABLE I

List of User Interview Questions

**Questions**

---

What is your writing background? What is your preferred genre and length of story (i.e. novel, short fiction)?

---

What is your level of familiarity with using AI tools during your writing process – in particular, text to image generative models?

---

If you have used these tools in the past, what are some challenges you encountered?

---

Are there specific types of images or visual styles that you find particularly inspiring for your writing?

---

How important is it for you that the generated images match the tone or mood of your story?

---

Can you describe a typical writing session? When and how do you incorporate visual inspiration into your writing process?

---

How much time are you willing to spend exploring and generating images as part of your brainstorming process?

---

Would you value a feature that allows you to save or export generated images for later use?

---

How much control do you want over the generated images? For example, would you prefer detailed settings to fine-tune/customize the appearance of characters or settings?

---

In addition to character and setting images, are there other ways you imagine AI could assist in developing your story (i.e. suggesting plot points or dialogue based on the generated images)?

---

How important is it for you to give feedback on the generated images? Would you prefer a system that refines its suggestions based on your feedback over time?

---

What type of feedback mechanisms (e.g., rating, good/bad) would you find most helpful?

---

For the second part of the interview, participants completed an image generation exercise using the DreamStudio application by Stability AI which provides an web interface for Stable Diffusion. The goal of this exercise was to understand how the participants could potentially integrate an image generation tool into their writing process.  The exercise simulated the process of designing prompts to generate a character, setting, or otherwise relevant image for their story. Participants were encouraged to **describe their thought process while completing this exercise.**

**To begin the exercise, participants were provided with a brief overview of the features of DreamStudio. They were, then, instructed to generate a character or setting for a pre-existing writing project or to use ChatGPT [35]** **to generate a plot idea in a specific genre then use that idea to generate character or setting ideas. This second option was intended to streamline the workflow of idea generation since this aspect of the writing process can ordinarily be time consuming.**

**Questions that were asked after this image generation exercise are detailed in Table II below.**

TABLE II

List of Questions For Part II of Interview

**Questions**

---

How closely do the generated images align with your initial vision?

---

Did any unexpected elements in the images spark new ideas for your character or setting?

---

How would you incorporate these generated images into your writing process? I.e. Would you use the images as reference points, scene inspiration, or as a starting point for further creative exploration.

---

What are some things you liked about using this tool?

---

What are some things you disliked about using this tool?

---

(Related?) Did you choose to use the advanced settings – why or why not?

---

What were some of the challenges of using a tool like this?

---

Did you find it easy to translate your ideas into prompts?

---

Were there limitations in the tool's output?

---

How could the tool be improved to better suit your creative needs?

---

How do you feel about integrating such a tool into your writing process? Do you see potential benefits?

---

Would you ever copy paste text directly from your story or poem into the prompt field and use that?

---

Would you prefer to use a website, desktop application, browser extension or some other form of this application (ideally so it won’t disrupt your creative workflow)?

---

1. Thematic Analysis

The three users who were interviewed all wrote fiction but two users also had experience writing poetry. One participant had written a novel but currently crafted storylines as a Dungeon Master for Dungeons & Dragons. They, therefore, focused on creating flexible narratives that could change abruptly to adapt to the gameplay.

Their responses during the interview were organised into three categories: how AI generated art can be helpful during the writing process, pain points in using an image generation tool and suggestions for features to implement in an image generation application for writers.

Some of the benefits of using AI generated art are that incorporating AI-generated art at the writing planning stage can facilitate idea generation. Another user expressed the desire to find realistic images of corporate environments to better position their characters within the narrative, using these visual cues as a foundation for triggering personal memories and experiences that inform their storytelling. AI is also useful for creating secondary or supporting characters, helping authors to avoid unintentionally mirroring real individuals. Another reason users turn to conversational AI models like ChatGPT is to overcome writer’s block via creative writing prompts provided by ChatGPT. Furthermore, images serve as both inspiration for scenes and as reference points—for instance, a picture of a haunted mansion might be used at the onset of writing to inspire the setting and continuously throughout the writing process. Additionally, writers use these images to enhance descriptions within the narrative and to aid players of games like Dungeons & Dragons in visualizing the story more vividly, thereby making the narrative more engaging and accessible.

**All three participants opted to generate images for a story idea they already possessed. They inputted a prompt and then, refined this prompt depending on how suitable the generated images were.  Fig. 1 and 2 are examples of images generated by user #2 for a setting and character that would be appropriate for the plot of a murder mystery novel set in South India.**

Fig 1. Screenshot of DreamStudio interface from interview with User #2 for setting image generation

Fig 2. Screenshot of DreamStudio interface from interview with User #2 for character image generation

During the image generation exercise, participants expressed some frustrations regarding using DreamStudio and common pain points have been listed in Table III below.

TABLE III

List of Pain Points of Users

**Pain Points**

---

**Prompt specificity and control**

---

**Experience using the user interface and learning curve associated with it**

---

**Adjusting details of character’s appearance**

---

**System limitations and constraints**

---

**Lack of clarity on content moderation**

---

**Lack of inclusivity and diversity**

---

The first pain point, prompt specificity and control, refers to users’ struggle to ascertain how specific prompts needed to be in order to adequately control the output as well as how to best structure the prompt (i.e. sentences vs keywords). They found it difficult to exclude unwanted elements (e.g., specific genders or architectural features) and to weight the importance of certain keywords in the prompts. For example, one user stated that it would take more effort to generate realistic images than to find them with Google image search since they primarily write realistic fiction and narrative poetry.

In fact, the inadequate guidance provided for new users to DreamStudio – the steep learning curve and lack of documentation made it difficult to effectively use the tool. Furthermore, a lack of understanding of the system’s limitations made users hesitant to adjust certain settings, such as the number of generation steps. An user expressed the need to understand how altering an advanced setting such as prompt strength would affect the image without having to read extensive documentation.

Another challenge users faced was in generating images of characters with particular skin tones and one user explained that this lack of inclusivity is a common problem they have encountered when using generative AI. They explained that the AI often struggles to provide images of non-conventionally attractive individuals who have specific features, such as disfigurements, requested by the user. Unfortunately, DreamStudio also did not appear to learn from previous inputs or user satisfaction, making it hard for users to iterate towards a desired outcome. Finally, there were issues with images being flagged as inappropriate without clear reasons, and difficulty in adjusting prompts to overcome these restrictions. Figure 3 illustrates this problem and how the user entered prompt “medieval human woman, grey skin, muscular, brunette” produced images flagged as inappropriate. There was no explanation indicated in the user interface as to why the images were flagged and this lack of clarity was frustrating to the user. It was also difficult to ascertain why these images were regarded as inappropriate because in the blurred outlines, the women appeared to be fully dressed and covered in armour.

Fig 3. Screenshot of DreamStudio interface with flagged images from interview with User #3 for character image generation

The third category, suggested features for an image generation application, is summarized in Table IV below. Some of these features are available in the DreamStudio application and participants mentioned how useful they were. Other features, such as prompt suggestions, were unavailable in DreamStudio and participants expressed the need for them to improve their user experience.

TABLE IV

List of Suggested Features For Image Generation Application

| Feature Category | Features |
| --- | --- |
| Control and Customization | Mid to high-level control over image details |
|  | Prompt suggestions and keyword recommendations |
|  | Option to adjust randomness in image generation |
|  | Selection of image style |
|  | Conversion of story text into prompts |
| User Interface and Accessibility | Website format for accessibility and login |
|  | Hover-over text for tool explanation |
|  | Queueing up prompts for batch image generation |
|  | Organizational features – i.e. folder creation, save/export image |
|  | Hover-over text for tool explanation |
| Feedback and Improvement | Non-intrusive, optional, feedback mechanism |
|  | Prompt-image history viewing for prompt refinement |
| Content Generation | Dialogue generation |
| Additional Features | Captions or descriptions alongside images |
|  | Moodboard |
1. Initial user requirements and site map

The interviews helped to better understand the needs of creative writers, and the initial list of user requirements developed as a result is shown in Table V below.

TABLE V

Initial List of User Requirements for Image Generation Application

| Category | User Requirements |
| --- | --- |
| Storyboarding and Mood-boarding | Storyboard to arrange images and add text for each scene |
|  | Moodboard feature for organizing/filtering content, specific to projects |
| Image Generation | Generation for both settings and characters, in realistic and fantastical/surrealist styles |
|  | Control over setting, character details, image style, mood/tone (e.g., color), and degree of randomness |
|  | Negative prompt field |
|  | Option to adjust weighting for keywords in prompt |
|  | Fixing physical appearance issues in generated characters |
| Advanced Settings and Customization | Advanced settings (generation steps, model, seed, etc.) as optional but with default values |
|  | Clarification on how settings affect images without extensive documentation |
|  | Caption generation for images with specific writing styles and tones |
|  | Image visualization map for clustering/sorting based on similarities |
| Feedback and Improvement | Unintrusive/optional feedback on images to improve system performance |
|  | Rating system for feedback with optional explanation space |
|  | Feedback leading to improved future prompt suggestions |
|  | Specific feedback on image aspects for correction in subsequent generations |
| Organization and Accessibility | Save, export, and organize images and prompts by projects |
|  | View prompt-image pairs (prompting history) |
|  | Creating folders in the app for organization |
|  | Hover-over text for tool explanation or interactive prompts for tool introduction |
|  | Login/account functionality |
|  | Application as a website |
|  | Logging user interactions to a database |
| Prompt Handling and Context | Suggestions for prompt expansion |
|  | Prompt suggestions based on existing project information and prompt-image history |
|  | Option to copy-paste prose directly into prompt field for conversion into a prompt |

Based on the extensive list from Table V, an rough sitemap (Figure 4) depicting the user flow through the application was developed.

Fig 4. Initial sitemap for image generation application

However, although the list of requirements and sitemap were comprehensive, it became evident that it was too unrealistic to attempt to implement all of these features during this project. In addition, there was not a clear research direction present. Hence, I decided to test out using multiple generative AI models for creative writing to assess the benefits, potential use cases, and challenges of working with the software.

1. Testing generative AI for writing tasks

To identify research challenges that could be addressed through this research project, I utilised GPT-4 by OpenAI [36], Claude-2-100k by Anthrophic [37], DALL·E 3 [38] and Stable Diffusion XL [5] to assist me with a series of writing tasks. Due to my BA English Literature and Creative Writing degree, I have experience writing novels, short stories, and poetry. I used Poe [39] by Quora since it hosts multiple LLMs and image generation models which can all be accessed with a single paid subscription.

1. Writing task #1: Generating images of scenes from a novel

I used GPT-4 to edit prompts that I used to generate images of a scene from the first draft of my fantasy novel using DALL·E 3. The instruction provided to GPT-4 was: “Convert the following scene from a book I'm writing into a text description for Dall E.” This instruction was combined with several paragraphs describing a scene in which the main character is walking through the woods carrying the body of her dead mother. GPT-4 responded with the following prompt for DALL·E: *A person carrying a heavy body through a serene forest, with trees full of quiet anticipation. The carrier is determined, despite the evident pain shown by a slight grimace indicating an ache in their back and shoulders. Beside them, a tall man with glasses matches their stride, his expression one of concern as he offers to help. The trees are animated by a gentle wind, and there's a subtle glow indicating a boundary ahead that separates the mystical city of Rapture from the mortal world. The atmosphere is one of urgency and tender care within a fantastical woodland setting.*

This prompt was then edited to specify the age and gender of the main character since this information was missing in the prompt. After my edits, this is the prompt that was entered into DALL·E: *A 16 year old Indian girl carrying the body of her dead mother through a serene forest, with trees full of quiet anticipation. The carrier is determined, despite the evident pain shown by a slight grimace indicating an ache in her back and shoulders. Beside her, a tall Indian boy with glasses matches her stride, his expression one of concern as he offers to help. The trees are animated by a gentle wind, and there's a subtle glow indicating a boundary ahead that separates the mystical city of Rapture from the mortal world. The atmosphere is one of urgency and tender care within a fantastical woodland setting.*

The resulting image, Figure 5, was beautifully rendered but several details were incorrect. I instructed GPT-4 to edit the prompt to emphasise that the girl should be carrying her dead mother and again, an incorrect image was generated. This iterative process of prompt refinement was over an hour in length and the model struggled to generate images that not only displayed the protagonist carrying her mother but also had the correct tone and aesthetic style.

Fig 5. Image of scene from fantasy novel generated using DALL·E 3

Another series of attempts to generate images for a more complex scene in which the characters are preparing to resurrect the protagonist’s dead mother in a forest revealed that DALL·E 3 struggles to render images containing multiple people accurately while adhering to specific details in the prompt. The character’s faces were rendered poorly, as shown in Figure 6. Moreover, DALL·E 3 becomes confused about clothing and ethnicity even if these are specified in the prompt. Generally, the background is the most useful part of the images that were generated and a single character in the forefront of the image is rendered more clearly.

Fig 6. Image of resurrection scene from fantasy novel generated using DALL·E 3

1. Writing task #2: Generating images of characters for fantasy novel

For this task, I used GPT-4 to convert a character description from my novel into a prompt for DALL·E 3 to generate images of the love interest, Akash. The model struggled to render his ethnicity and tattoos correctly, two of the more identifiable features. However, with more round of iteratively editing the prompt, the results improved significantly. I also tested out using Claude to help create prompts for Stable Diffusion but the limits on prompt length for Stable Diffusion made it difficult to generate more accurate character portraits.

Fig 7. Image of character, Akash, from fantasy novel generated using DALL·E 3

Fig 8. Image of character, Akash, from fantasy novel generated using Stable Diffusion XL

- Writing task #3: Generating images of objects for fantasy novel

Generating images of objects using DALL·E 3 had promising results. The details were helpful - the tattered pages, unintelligible script, warm lighting and bizarre illustrations in particular proved very inspiring.

Fig 9. Image of witch’s grimoire from fantasy novel generated using DALL·E 3

1. Writing task #4: Generating images of settings for fantasy novel

Another task that had useful results was generating images of settings using DALL·E 3 and Stable Diffusion.

Fig 10. Image #1 of witch’s magic shop from fantasy novel generated using DALL·E 3

Fig 11. Image #2 of witch’s magic shop from fantasy novel generated using DALL·E 3

1. Conclusions from using generative AI

Table VI describes a list of tasks that were successful and could feasibly be implemented in an application. For some of these tasks, more context is needed and to generate more diverse characters, using an LLM to edit prompts to emphasise gender or physical features is crucial for improving the output.

TABLE VI

Image Generation and Creative Writing Tasks Successfully Implemented With DALL·E 3

| Task | Description | Purpose in Creative Writing |
| --- | --- | --- |
| Converting Text to Image Prompts | Transforming narrative text or descriptions from a story into visual image prompts | Helps visualize scenes, characters, and settings directly from the written content, enhancing the writer's imagination and clarity |
| Generating Recommended Image Prompts | Providing suggestions for additional image prompts based on previously used ones, tailored to the project's theme. E.g. recommending prompts related to Japanese culture for a fantasy story featuring Sakura trees, samurai, and Asian-style dragons | Assists in maintaining thematic consistency and expanding the visual elements of a story, offering new perspectives and ideas |
| Creating Diverse and Unique Characters | Generating images of characters that are diverse and avoid clichés or stereotypes, such as characters with scars or unique features | Enables the portrayal of a wide range of characters, fostering inclusivity in character development |
| Crafting Mood Boards | Developing mood boards that visually convey specific emotions or themes, assisting in the establishment of a story's tone | Provides a visual collection of elements that capture the mood or theme of a story, aiding in the development of atmosphere and narrative tone |

However, one of the primary challenges when using DALL·E 3 to generate images for scenes is its tendency to inaccurately represent the number of people, alter their appearances, duplicate characters, and overlook finer details like specific objects. In addition, manually editing prompts and typing out specific instructions to GPT-4 to aid in prompt refinement is time consuming and tedious.

1. Updated user requirements

The updated list of user requirements are organised by research and implementation requirements and detailed in Table VII and VIII below. Among these requirements, addressing writer's block appears to be the most promising research direction for this project. Furthermore, the application will support brainstorming and outlining through storyboarding, image search, and generation features.

TABLE VII

Research Requirements for Image Generation Application

| Category | Research Requirements |
| --- | --- |
| Brainstorming and Outlining | Images should aid in the brainstorming and outlining stages of writing |
|  | Images of characters and settings should immerse the writer in the story, serving as inspiration and reference points |
| Detail Control and Customization | Ability to control specific details in images, such as adding objects or traits to settings and characters |
| Inclusivity and Diversity | Character art should be inclusive and diverse, avoiding stereotypes or clichés |
| Addressing Writer’s Block | Images accompanied with dialogue, plot points, or character descriptions which act as writing prompts |
|  | Images which inspire plot twists or promote freewriting exercises |
| Efficiency in Image Generation | Quick and easy generation of images |
|  | Images should align with the writer’s vision with minimal trial and error. |
| Feedback and Improvement | Unintrusive and/or optional feedback to improve system performance |
|  | Feedback to improve prompt suggestions, image quality, and relevance |
|  | Option for users to specify correct and incorrect aspects of an image for more accurate future generations |
| Storyboarding for Plot Development | Storyboard feature for plotting stories in genres like realistic literary fiction and fantasy |
|  | Ability to explore themes, character arcs, storylines, and worldbuilding (e.g., magic systems, fantasy maps) through a combination of imagery and written descriptions |

TABLE VIII

Implementation Requirements for Image Generation Application

| Category | Implementation Requirements |
| --- | --- |
| Integration with Writing Process | The application integrates a prompt engineering workflow with the writer's existing writing process |
|  | Allows users to copy and paste text directly into the prompt field with minimal editing |
| Image Generation Control | Control over image style and mood |
|  | Control over the degree of randomness in images |
|  | Advanced settings (generation steps, model, seed, etc.) as optional but with default values |
|  | Ability to generate 1 to 4 images |
|  | Option to adjust positive and negative weighting for different keywords in prompt |
|  | Mechanism to correct issues with physical appearance in generated images |
|  | Clarity on reasons for images being flagged as inappropriate and alternative suggestions |
|  | Clear explanation of how advanced settings affect images |
| Prompt Handling | Suggestions for expanding both regular and negative prompts with keyword recommendations |
| User Interaction and Feedback | Logging user interactions to a database for a prompt-image history log and intelligent future prompt suggestions |
| Image Display and Tracking | Ability to view prompt-image pairs and group images by prompt to track progression |
|  | Functionality to click on an image to expand it for a larger view with details about the prompt, seed, and other generation parameters |

Do I need to define what this is and cite a resource on it?

**Ask Kai whether the sitemap should be included because it’s not particularly good and the final application won’t have many of these features (which I also note in my report but still).**

## References

[1]  K. Kenthapadi, H. Lakkaraju, and N. Rajani, ‘Generative AI meets Responsible AI: Practical Challenges and Opportunities’, in *Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, in KDD ’23. New York, NY, USA: Association for Computing Machinery, Aug. 2023, pp. 5805–5806. doi: 10.1145/3580305.3599557.

[2]  A. Yuan, A. Coenen, E. Reif, and D. Ippolito, ‘Wordcraft: Story Writing With Large Language Models’, in *27th International Conference on Intelligent User Interfaces*, in IUI ’22. New York, NY, USA: Association for Computing Machinery, Mar. 2022, pp. 841–852. doi: 10.1145/3490099.3511105.

[3]  P. Mirowski, K. W. Mathewson, J. Pittman, and R. Evans, ‘Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals’, in *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*, in CHI ’23. New York, NY, USA: Association for Computing Machinery, Apr. 2023, pp. 1–34. doi: 10.1145/3544548.3581225.

[4]  H. Zhu *et al.*, ‘End-to-end Story Plot Generator’. arXiv, Oct. 12, 2023. doi: 10.48550/arXiv.2310.08796.

[5]  ‘Generative Models by Stability AI’. Stability AI, Oct. 31, 2023. Accessed: Nov. 01, 2023. [Online]. Available: https://github.com/Stability-AI/generative-models

[6]  Y. Feng *et al.*, ‘PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation’. arXiv, Aug. 15, 2023. doi: 10.48550/arXiv.2307.09036.

[7]  ‘Large-scale Text-to-Image Generation Models for Visual Artists’ Creative Works | Proceedings of the 28th International Conference on Intelligent User Interfaces’. Accessed: Jan. 29, 2024. [Online]. Available: https://dl.acm.org/doi/abs/10.1145/3581641.3584078

[8]  V. Liu, H. Qiao, and L. Chilton, ‘Opal: Multimodal Image Generation for News Illustration’, in *Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology*, in UIST ’22. New York, NY, USA: Association for Computing Machinery, Oct. 2022, pp. 1–17. doi: 10.1145/3526113.3545621.

[9]  P. Chambon, C. Bluethgen, C. P. Langlotz, and A. Chaudhari, ‘Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains’. arXiv, Oct. 08, 2022. doi: 10.48550/arXiv.2210.04133.

[10]  I. Gilburt, ‘A machine in the loop: the peculiar intervention of artificial intelligence in writer’s block’, *New Writ.*, vol. 0, no. 0, pp. 1–12, 2023, doi: 10.1080/14790726.2023.2223176.

[11]  K. I. Gero, T. Long, and L. B. Chilton, ‘Social Dynamics of AI Support in Creative Writing’, in *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*, in CHI ’23. New York, NY, USA: Association for Computing Machinery, Apr. 2023, pp. 1–15. doi: 10.1145/3544548.3580782.

[12]  M. Kreminski and C. Martens, ‘Unmet Creativity Support Needs in Computationally Supported Creative Writing’, in *Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022)*, T.-H. ‘Kenneth’ Huang, V. Raheja, D. Kang, J. J. Y. Chung, D. Gissin, M. Lee, and K. I. Gero, Eds., Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 74–82. doi: 10.18653/v1/2022.in2writing-1.11.

[13]  E. Jiang *et al.*, ‘PromptMaker: Prompt-based Prototyping with Large Language Models’, in *Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems*, in CHI EA ’22. New York, NY, USA: Association for Computing Machinery, Apr. 2022, pp. 1–8. doi: 10.1145/3491101.3503564.

[14]  S. Brade, B. Wang, M. Sousa, S. Oore, and T. Grossman, ‘Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models’. arXiv, Apr. 18, 2023. doi: 10.48550/arXiv.2304.09337.

[15]  M. Ruskov, ‘Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales’. arXiv, Aug. 25, 2023. doi: 10.48550/arXiv.2302.08961.

[16]  E. Bensaid, M. Martino, B. Hoover, and H. Strobelt, ‘FairyTailor: A Multimodal Generative Framework for Storytelling’. arXiv, Jul. 12, 2021. doi: 10.48550/arXiv.2108.04324.

[17]  N. Singh, G. Bernal, D. Savchenko, and E. L. Glassman, ‘Where to Hide a Stolen Elephant: Leaps in Creative Writing with Multimodal Machine Intelligence’, *ACM Trans. Comput.-Hum. Interact.*, vol. 30, no. 5, p. 68:1-68:57, Sep. 2023, doi: 10.1145/3511599.

[18]  ‘OpenAI Platform’, OpenAI API Models. Accessed: Jan. 30, 2024. [Online]. Available: https://platform.openai.com/docs/models/overview

[19]  ‘TaleBrush: Sketching Stories with Generative Pretrained Language Models | Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems’. Accessed: Jan. 29, 2024. [Online]. Available: https://dl.acm.org/doi/10.1145/3491102.3501819

[20]  H. Guinness, ‘The best AI writing generators in 2023’, Zapier. Accessed: Oct. 31, 2023. [Online]. Available: https://zapier.com/blog/best-ai-writing-generator/

[21]  ‘Sudowrite’, Sudowrite. Accessed: Oct. 31, 2023. [Online]. Available: https://www.sudowrite.com/

[22]  ‘New beta feature: Canvas’, Sudowrite. Accessed: Nov. 01, 2023. [Online]. Available: https:///changelog/16456, https://feedback.sudowrite.com/changelog/16456

[23]  Patil, Pedro Cuenca, and Valentine Kozin, ‘Training Stable Diffusion with Dreambooth using Diffusers’, Hugging Face. Accessed: Nov. 01, 2023. [Online]. Available: https://huggingface.co/blog/dreambooth
