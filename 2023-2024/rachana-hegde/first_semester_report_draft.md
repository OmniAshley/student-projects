![image](https://github.com/Vis4Sense/student-projects/assets/66835338/3b3c030f-6c77-419c-9498-9f42b0a5b76e)Note: I'm aware that the formatting is incorrect and I will fix this as soon as I finish writing the draft. 

# First Semester Report

## Abstract
TBD

## Index Terms

TBD

## I. Introduction 
**(1100 words)**

Generative  AI is altering conventional approaches to working in multiple industries by creating a paradigm shift through automation and enhanced productivity. Applications for generative AI include email writing, teaching assistance, graphic design, code synthesis, facilitation of drug exploration, copywriting, virtual assistance and content creation [1]. Large language models (LLMs), in particular, such as GPT-3, generate text and carry out tasks such as writing code and autocompletion, often without requiring specialised training [2]. LLMs are also useful for natural language processing tasks such as script and plot generation [2], [3], [4]. Generative image models such as Stable Diffusion [5] are trained to denoise images with guidance from prompts, thereby producing new images [6]. Text-to-image generative models have varied use cases from facilitating industrial design [7] to news illustration [8] and producing images for radiology [9].

This research project explores using generative AI to assist creative writers with brainstorming. The following paragraphs discuss the creative needs of writers, challenges of applying diffusion models to writing tasks and an proposed solution, and the potential contributions of this research project. ****

Writers often utilize Pinterest to find visual inspiration, curating vision boards that evoke the aesthetics and themes of their story. However, it is difficult to find images that precisely capture their vision. This problem is twofold because firstly, it is challenging to find artwork for characters that are fantastical creatures with unconventional appearances. Secondly, even if the character possesses a more generic appearance, it is unlikely that there will be images depicting that character in a specific setting from the story. Traditional solutions for this involve commissioning custom artwork, which is expensive, and searching for images on various websites, which is time-consuming and tedious.

This project addresses this problem by designing an image generation application that writers can use to generate reference images for characters. Some of the reasons that the images generated by an image generation model such as Dall E are not always useful is that the model will incorrectly render character’s appearances and duplicate or eliminate individuals/objects from the image. This can be an issue if there are several characters who are important to a scene and must be present in the image but are missing or unrecognizable.

Even after the model generates aesthetically pleasing images that fulfil the writer’s vision, it will not ‘remember’ the character’s appearance. Hence, the character’s appearance will be inconsistent across different settings and scenarios. Addressing this problem and ensuring that the main cast of characters appears the same in each generated image would make it easier for the writer to visualise scenes in their book. This visual consistency will also be useful for storyboarding because it will make the connections between scenes clearer and the writer can identify issues with the narrative arc (i.e. plot holes or incomplete storylines).

Writers often struggle to decide which ideas to pursue but visualising plot threads as sequence of images could aid in making narrative decisions. This, in turn, will enable this project to address the core research requirement of assisting writers in overcoming writer’s block that can result from lack of inspiration or the struggle to plan out a story sufficiently. Hence, the goal of this research project is to build an application for creating a storyboard using images generated by AI to assist writers with outlining their narrative.

This research project seeks to improve the visual consistency of images produced for storyboarding by optimising prompts with a pretrained LLM (GPT-4) for a diffusion model finetuned on a small dataset of character art. The project will explore the extent to which applying these existing methods of prompting and finetuning can address the limitations of image generation models described above.

One of the motivations for this project is that there are currently no comprehensive and easy to follow/implement guides available to individuals who want to use and customise image generation models. While there are forums such as Reddit Stable Diffusion where anonymous users can discuss the results of experimenting with machine learning models, these forum posts can often be convoluted and difficult to make sense of without the requisite knowledge of machine learning. Implementing the advice found in Medium articles and Twitter posts can also be time consuming and mentally exhausting as this process can involve a great deal of trial and error, demanding hours of debugging error messages full of machine learning jargon.

In summary, this research project aims to make the following contributions:

1. The application will have an interface for user to connect scenes (containing text and images) together to make the storyboard and assist them in outlining.
2. The application will also support writers in finetuning a diffusion model (assuming they have limited knowledge on generative AI). It will facilitate the process for designing characters, producing dataset(s) of character art and training the image generation model on the dataset(s) to produce more relevant images for constructing the storyboard. It will do so by optimising prompts for the diffusion model and facilitating the image generation process to reduce the mental burden on the writer (i.e. by providing advice and guidance at step of the process).
3. The image generation will also provide visual inspiration for character design.
4. The combination of plot ideas (from LLM) and more relevant image generation will collectively help address writer’s block.
5. The project will explore limitations of diffusion models and how these can be circumvented using methods for customising diffusion models. The prototype evaluation will demonstrate the extent to which these customisation methods work for this specific use case of assisting writers with storyboarding and detail how code provided by past researchers was modified to enhance the results from finetuned models.

The following section will explore relevant literature related to prompt optimisation tools, using image generation for story visualisation, writer’s block, using LLMs for creative writing and finetuning diffusion models. Section III will, then, describe how the scope of this research project was refined through the thematic analysis of initial user interviews and the resulting user requirements for the application. Section IV further explores the application design and justifications for utilising the chosen LLM and image generation models. Finally, Section V. will conclude with an overview of future work for this project including building, testing, and evaluating the prototype and the updated project timeline.

## II. Literature Review
**2051 words**

This literature review highlights the origins and mitigation of writer's block in section II-A alongside exploring generative AI potential for aiding in overcoming writer's block. Section II-B is, then, concerned with recent research on incorporating AI into storyboarding software and section II-C covers prompt engineering for generative image models. Section II-D establishes some recent advancements in story visualisation and section II-E investigates the integration of LLMs into writing software. Finally, Section II-F surveys finetuning methods and their limitations. This literature review informs key decisions for the application design.

**Writer’s block and creative needs**

First, it is important to understand the origins of writer’s block and how it can be mitigated. Writer’s block occurs when a skilled writer is unable to generate fresh work for a set amount of time [10]. Contributing factors to writer’s block include stress, depression, diminished motivation, perfectionism, procrastination [11]. According to Ahmed and Güss, writers struggle at various stages in the writing journey such as selecting ideas to develop further, brainstorming, and expressing ideas [11]. By building image generation and storyboarding software, this project seeks to counteract problems such as ‘overplanning’ which Ahmed and Güss describe as deciding a certain outcome for a creative endeavour too early without sufficiently appraising other possibilities [11]. This obsession with a particular solution combined with inflexible thinking can obstruct creative thought processes [11]. This project hypothesizes that accessing a tool that actively encourages writers to view multiple plotlines and branching narratives will foster a more adaptable mindset early on.

To determine how to design an application that will address writer’s block, it is necessary to explore some literature that discusses this phenomenon in relation to generative AI. Namely, Gero et al. conducted interviews with creative authors and shared their findings on how AI writing tools can best support them [12]. Gero et al. note that it would be helpful to manage the writer’s expectations regarding the AI system’s abilities by providing guidance in the form of context, examples of how the AI system will perform for various tasks, and the interface design which separates AI generated text from story inputted by the user [12].

When designing an application for writers, it is also important to recognise the types of support that would be helpful. This research project will target the planning stage of writing which is highly conceptual, involving establishing objectives and developing ideas [12]. For planning, Gero et al. record that writers expressed the desire for inspiration and assistance with writer’s block – particularly when they had difficulty making narrative decisions, they would like to choose from multiple scenarios and/or adapt these computer-generated ideas [12]. An earlier paper also highlights that writers require assistance with establishing plot, ensuring narrative consistency, and creating compelling story arcs [13].

Kreminski and Martens further explain that the writer’s goal is to communicate a cognitive framework for the narrative’s world which contains a cast of character, objects, settings and relationships between these elements which evolve over time [13]. When the narrative grows increasingly complex, it becomes imperative for writers to record details to ensure that there are no contradictions, discrepancies, or plot holes in the story’s chronology [13].  To achieve narrative consistency, writers require storyboards to examine the narrative at a conceptual level.

When considering how to design the storyboard functionality for the application in this research project, Kreminski and Martens’ discussion of models for plotting including the Hero’s Journey and Freytag pyramid are useful to consider to design the template [13]. In fact, Kreminski and Martens break down the plot into the following scenes: the inciting incident that motivates the characters, the character’s journey in trying to address this event, the climax and resolution or aftermath [13].

Thus, this literature will help inform design decisions for the application design.

1. **AI Storyboarding software**

There is less recent research on integrating storyboarding software with AI tools specifically for writing. However, Jo et al. built Gennie which takes user-entered text as input and outputs sketches of scenes from the story [14]. Unlike Gennie which produces ‘draft sketches’ with less detail [14], this project’s objective is to produce higher quality images with greater fidelity to text prompts. More recently, Lee explored using ChatGPT and Midjourney to enhance results from using AiSAC or ‘AI Analysis System for Ad Creation’ which is an AI powered storyboarding tool [15].

### **C. Prompt recommendation tools**

A crucial first step in using generative image models, such as diffusion models, is providing effective prompts. According to Jiang et. al [16], some challenges of prompt engineering include finding examples for prompts, troubleshooting and assessing if prompts are improving. To enhance prompt engineering for non-technical users, research has investigated user-friendly approaches for crafting effective prompts for generative image models. For example, Promptify provides prompt recommendations for text to image models using the LLM GPT-3.5 to suggest keywords and facilitating the organisation of generated images [17]. In contrast, PromptMagician utilises the DiffusionDB dataset’s collection of prompts and images to produce keyword recommendations and filters resulting images [6]. Similarly, RePrompt automatically updates prompts to improve the accuracy of emotions conveyed in images [18].

This project implicitly addresses challenges that users, who are unfamiliar with machine learning models, encounter when attempting to articulate their preferences and requirements to generative AI using prompts. To accomplish this, the research will implement recommendations provided in these papers, including using advanced models such as GPT-4 for prompting, adopting chain of thought prompting for prompt clarification, introducing negative prompts, and the enhancement of user control over multiple hyper-parameters [6], [17].

### **D. Sequential image generation for illustrating stories**

Multiple research papers discuss various methods for story visualisation using a combination of LLMs and image generation models [19], [20], [21]. Namely, Kumagai et al. focus on converting stories into logical visual sequences by addressing problems such as contextual accuracy and consistency [22]. Kumagai et al.’s study [22] uses an LLM to add information about context and characters to each prompt. They experimented with movie scripts to test their method, discovering that it enhances the coherency and contextual authenticity of outputted images. While Kumagai  et al.’s approach is promising, it involved utilising an AR-LDM, an autoregressive latent diffusion model [22]. AR-LDM can convey the connections between sequential images by accounting for captions and previously generated images but it can be confused by unfamiliar character names and pictures might not adhere to the narrative’s flow [22]. Kumagai  et al. used AR-LDM to train a model on ‘character-specific tokens’ to enable the model to obtain information regarding the subject’s physical characteristics from inputted prompts [22]. Although this project will not use AR-LDM, Kumagai  et al.'s research method of modifying prompts by adding contextual factors such as physical qualities of characters to increase consistency of character appearance in different photos is relevant to this project [22]. Hence, the prompt design strategies in this paper [22] were used as inspiration for the methodology.

Previous research has also investigated the use of generative image models to illustrate fairy tales. For instance, Ruskov discusses using Midjourney v4 to create illustrations for fairytales and devising an prompt refinement process for image creation [23]. Another example is FairyTailor, a platform for blending generated text with images from Unsplash to construct children’s fairytales and offers features such as auto-completion [24].

### **E. LLMs for creative writing**

Multiple papers have discussed building writing tools powered by AI and how these can automate parts of the writing process. For example, Dramatron can produce scripts and screenplays by using prompt chaining to create structural context [3]. The method explored in this paper [3], termed ‘hierarchical story generation’, can provide inspiration for working with an LLM during this research project. The paper describes how a log line, which condenses the narrative into a few sentences with information about the setting, protagonist, and conflict, is used to generate the story [3]. In order to build the plot ideas generation feature, it will be necessary to use few shot learning and chain of thought prompting to make OpenAI API calls. This research project will also use one sentence plot summaries as narrative context to guide plot idea generation from the LLM. While the precise prompts from the Dramatron paper [3] will not be applicable, understanding how they generated a cohesive script through prompt programming will be valuable for determining how to provide more relevant plot suggestions to users of the application developed during this research project. However, one of the limitations that was flagged in [3] is that LLMs’ context window is restricted to 2048 tokens in advanced models and this is no longer applicable. OpenAI’s GPT-4-turbo models, for instance, have context windows of up to 128,000 tokens [25] and as this context window continues to increase in the future – it will enable LLMs to produce more accurate plot suggestions based on larger amounts of information about the story.

Two papers that explore using LLMs to augment the writing process, [26] and [2], develop applications with writing interfaces that integrate writing suggestions generated by AI. Wordcraft, for example, employs a Language Model (LLM) in conjunction with an editor and an interface that prompts users to extend the narrative, propose rewrites, and request additional details [2]. Wordcraft can be used for tasks such as generating ideas, rewriting scenes and copyediting [2]. Singh et al., similarly, designed an interface that also provides images from Unsplash and audio from Freesound to further enhance the writing process [26].

Another program, End-to-End Story Plot Generator, can produce a plot using LLaMA2 which removes the rate limit associated with OpenAI API calls and allows for large-scale text generation [4]. Since this paper [4] involves finetuning multiple LLMs and building datasets, for training the models, to improve the quality of the plots generated – it wouldn’t be feasible to implement their entire solution. However, the information in this paper [4] about designing and structuring effective prompts to generate details about premise and characters using an LLM is useful. For example, they note that LLaMA2 emphasised the character’s appearance, so they modified the prompt to ensure that the LLM’s output contained information about the character’s profession and connections to other characters [4].

Other research has explored enabling writers to influence story outline generation by drawing lines that reflect the protagonist’s fate [27] and Yuan et al. incorporate a chatbot which writers can use for generating ideas or conducting research [2].  Furthermore, there are also publicly available AI writing tools for use cases such as copywriting and marketing [28]. To that end, a fiction writing tool called Sudowrite uses Transformer models such as GPT-3 by OpenAI and the diffusion model DALL·E to generate images, text autocompletions, writing feedback, and plot ideas [29], [30].

### **F. Personalising diffusion models**

This section will delve into methods for finetuning diffusion models. Models such as Stable Diffusion [5] can be customised using techniques such as Dreambooth [31],  LoRA [32], Custom Diffusion [33], and Textual Inversion [34]. Dreambooth addresses the inability of pre-trained text-to-image models to replicate subjects from a dataset and create new representations of them in various situations [31]. With Dreambooth, the model requires 3-5 images of a subject to produce images of the subject in new settings by learning to link an distinctive identifier with a particular subject [31]. Some scenarios in which Dreambooth fails include: unfamiliar environments described in prompts, overfitting where photos identical to the training set are produced, rare subjects, and hallucinations [31].

One criticism of the LoRA, Dreambooth, and Textual Inversion fine tuning methods is that they struggle to recreate more than one subject [33]. Custom Diffusion seeks to overcome this constraint and Kumari et al. assert that tuning a few parameters in the ‘text-to-image conditioning mechanism’ allows the model to generate images of multiple subjects [33]. This training can involve merging multiple tuned models through ‘closed-form constrained optimization’ or training for several entities simultaneously [33]. Benefits of using Custom Diffusion include the 6 minute training time which is much shorter than Dreambooth and that it consumes less resources such as memory and model storage space [33]. Despite its benefits,  Custom Diffusion still has difficulty producing images with three or more specific subjects [33].  This research project aims to develop a better understanding of limitations associated with methods such as Dreambooth and Custom Diffusion and potentially discover ways to circumvent them by leveraging recent modifications in the training scripts.

## III. User Requirements
**3700 words including tables**


Fig 3. Screenshot of DreamStudio interface with flagged images from interview with User #3 for character image generation

The third category, suggested features for an image generation application, is summarized in Table IV below. Some of these features are available in the DreamStudio application and participants mentioned how useful they were. Other features, such as prompt suggestions, were unavailable in DreamStudio and participants expressed the need for them to improve their user experience.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/6f72ca84-670c-4a4a-ba35-48571350322d/5c019a0d-b1fc-4465-89f7-eddfbd4d1c14/Untitled.png)

## References

[1]  K. Kenthapadi, H. Lakkaraju, and N. Rajani, ‘Generative AI meets Responsible AI: Practical Challenges and Opportunities’, in *Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, in KDD ’23. New York, NY, USA: Association for Computing Machinery, Aug. 2023, pp. 5805–5806. doi: 10.1145/3580305.3599557.

[2]  A. Yuan, A. Coenen, E. Reif, and D. Ippolito, ‘Wordcraft: Story Writing With Large Language Models’, in *27th International Conference on Intelligent User Interfaces*, in IUI ’22. New York, NY, USA: Association for Computing Machinery, Mar. 2022, pp. 841–852. doi: 10.1145/3490099.3511105.

[3]  P. Mirowski, K. W. Mathewson, J. Pittman, and R. Evans, ‘Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals’, in *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*, in CHI ’23. New York, NY, USA: Association for Computing Machinery, Apr. 2023, pp. 1–34. doi: 10.1145/3544548.3581225.

[4]  H. Zhu *et al.*, ‘End-to-end Story Plot Generator’. arXiv, Oct. 12, 2023. doi: 10.48550/arXiv.2310.08796.

[5]  ‘Generative Models by Stability AI’. Stability AI, Oct. 31, 2023. Accessed: Nov. 01, 2023. [Online]. Available: https://github.com/Stability-AI/generative-models

[6]  Y. Feng *et al.*, ‘PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation’. arXiv, Aug. 15, 2023. doi: 10.48550/arXiv.2307.09036.

[7]  ‘Large-scale Text-to-Image Generation Models for Visual Artists’ Creative Works | Proceedings of the 28th International Conference on Intelligent User Interfaces’. Accessed: Jan. 29, 2024. [Online]. Available: https://dl.acm.org/doi/abs/10.1145/3581641.3584078

[8]  V. Liu, H. Qiao, and L. Chilton, ‘Opal: Multimodal Image Generation for News Illustration’, in *Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology*, in UIST ’22. New York, NY, USA: Association for Computing Machinery, Oct. 2022, pp. 1–17. doi: 10.1145/3526113.3545621.

[9]  P. Chambon, C. Bluethgen, C. P. Langlotz, and A. Chaudhari, ‘Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains’. arXiv, Oct. 08, 2022. doi: 10.48550/arXiv.2210.04133.

[10]  I. Gilburt, ‘A machine in the loop: the peculiar intervention of artificial intelligence in writer’s block’, *New Writ.*, vol. 0, no. 0, pp. 1–12, 2023, doi: 10.1080/14790726.2023.2223176.

[11]  K. I. Gero, T. Long, and L. B. Chilton, ‘Social Dynamics of AI Support in Creative Writing’, in *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*, in CHI ’23. New York, NY, USA: Association for Computing Machinery, Apr. 2023, pp. 1–15. doi: 10.1145/3544548.3580782.

[12]  M. Kreminski and C. Martens, ‘Unmet Creativity Support Needs in Computationally Supported Creative Writing’, in *Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022)*, T.-H. ‘Kenneth’ Huang, V. Raheja, D. Kang, J. J. Y. Chung, D. Gissin, M. Lee, and K. I. Gero, Eds., Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 74–82. doi: 10.18653/v1/2022.in2writing-1.11.

[13]  E. Jiang *et al.*, ‘PromptMaker: Prompt-based Prototyping with Large Language Models’, in *Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems*, in CHI EA ’22. New York, NY, USA: Association for Computing Machinery, Apr. 2022, pp. 1–8. doi: 10.1145/3491101.3503564.

[14]  S. Brade, B. Wang, M. Sousa, S. Oore, and T. Grossman, ‘Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models’. arXiv, Apr. 18, 2023. doi: 10.48550/arXiv.2304.09337.

[15]  M. Ruskov, ‘Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales’. arXiv, Aug. 25, 2023. doi: 10.48550/arXiv.2302.08961.

[16]  E. Bensaid, M. Martino, B. Hoover, and H. Strobelt, ‘FairyTailor: A Multimodal Generative Framework for Storytelling’. arXiv, Jul. 12, 2021. doi: 10.48550/arXiv.2108.04324.

[17]  N. Singh, G. Bernal, D. Savchenko, and E. L. Glassman, ‘Where to Hide a Stolen Elephant: Leaps in Creative Writing with Multimodal Machine Intelligence’, *ACM Trans. Comput.-Hum. Interact.*, vol. 30, no. 5, p. 68:1-68:57, Sep. 2023, doi: 10.1145/3511599.

[18]  ‘OpenAI Platform’, OpenAI API Models. Accessed: Jan. 30, 2024. [Online]. Available: https://platform.openai.com/docs/models/overview

[19]  ‘TaleBrush: Sketching Stories with Generative Pretrained Language Models | Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems’. Accessed: Jan. 29, 2024. [Online]. Available: https://dl.acm.org/doi/10.1145/3491102.3501819

[20]  H. Guinness, ‘The best AI writing generators in 2023’, Zapier. Accessed: Oct. 31, 2023. [Online]. Available: https://zapier.com/blog/best-ai-writing-generator/

[21]  ‘Sudowrite’, Sudowrite. Accessed: Oct. 31, 2023. [Online]. Available: https://www.sudowrite.com/

[22]  ‘New beta feature: Canvas’, Sudowrite. Accessed: Nov. 01, 2023. [Online]. Available: https:///changelog/16456, https://feedback.sudowrite.com/changelog/16456

[23]  Patil, Pedro Cuenca, and Valentine Kozin, ‘Training Stable Diffusion with Dreambooth using Diffusers’, Hugging Face. Accessed: Nov. 01, 2023. [Online]. Available: https://huggingface.co/blog/dreambooth
