Note: I'm aware that the formatting is incorrect and I will fix this as soon as I finish writing the draft. 

# First Semester Report

## Abstract

- This research project explores using generative AI, such as LLMs and text-to-image generative models, to assist creative writers with brainstorming and storyboarding.
- **Note: Rewrite this after writing the introduction**

## Index Terms

TBC

## I. Introduction

Generative  AI is altering conventional approaches to working in multiple industries by creating a paradigm shift through automation and enhanced productivity. Applications for generative AI include email writing, teaching assistance, graphic design, code synthesis, facilitation of drug exploration, copywriting, virtual assistance and content creation [1]. Large language models (LLMs), in particular, such as GPT-3, generate text and carry out tasks such as writing code and autocompletion, often without requiring specialised training [2]. LLMs are also useful for natural language processing tasks such as script and plot generation [2], [3], [4]. Generative image models such as Stable Diffusion [5] are trained to denoise images with guidance from prompts, thereby producing new images [6]. Text-to-image generative models have varied use cases from facilitating industrial design [7] to news illustration [8] and producing images for radiology [9].

This research project explores using generative AI to assist creative writers with brainstorming. The following paragraphs discuss the creative needs of writers, challenges of applying diffusion models to writing tasks and an proposed solution, and the potential contributions of this research project.

Writers often utilize Pinterest to find visual inspiration, curating vision boards that evoke the aesthetics and themes of their story. However, it is difficult to find images that precisely capture their vision. This problem is twofold because firstly, it is challenging to find artwork for characters that are fantastical creatures with unconventional appearances. Secondly, even if the character possesses a more generic appearance, it is unlikely that there will be images depicting that character in a specific setting from the story. Traditional solutions for this involve commissioning custom artwork, which is expensive, and searching for images on various websites, which is time-consuming and tedious.

This project addresses this problem by designing an image generation application that writers can use to generate reference images for characters. Some of the reasons that the images generated by an image generation model such as Dall E are not always useful is that the model will incorrectly render character’s appearances and duplicate or eliminate individuals/objects from the image. This can be an issue if there are several characters who are important to a scene and must be present in the image but are missing or unrecognizable. (**Note:** Include a figure with an image example from my own experiment with Dall E – include the prompt in the caption – and specify that image was generated using Dall E on Poe (cite)).

Even after the model generates aesthetically pleasing images that fulfil the writer’s vision, it will not ‘remember’ the character’s appearance. **(Note: Include figure showing different images of same character with same prompt and specify that this was produced each time the generate button was clicked.**) Hence, the character’s appearance will be inconsistent across different settings and scenarios. Addressing this problem and ensuring that the main cast of characters appears the same in each generated image would make it easier for the writer to visualise scenes in their book. This visual consistency will also be useful for storyboarding because it will make the connections between scenes clearer and the writer can identify issues with the narrative arc (i.e. plot holes or incomplete storylines).

Writers often struggle to decide which ideas to pursue but visualising plot threads as sequence of images could aid in making narrative decisions. This, in turn, will enable this project to address the core research requirement of assisting writers in overcoming writer’s block that can result from lack of inspiration or the struggle to plan out a story sufficiently. Hence, the goal of this research project is to build an application for creating a storyboard using images generated by AI to assist writers with outlining their narrative.

This research project seeks to improve the visual consistency of images produced for storyboarding by optimising prompts with a pretrained LLM (GPT-4) for a diffusion model finetuned on a small dataset of character art. The project will explore the extent to which applying these existing methods of prompting and finetuning can address the limitations of image generation models described above.

One of the motivations for this project is that there are currently no comprehensive and easy to follow/implement guides available to individuals who want to use and customise image generation models. While there are forums such as **Reddit stable diffusion (cite)** where anonymous users can discuss the results of experimenting with machine learning models, these forum posts can often be convoluted and difficult to make sense of without the requisite knowledge of machine learning. Implementing the advice found in Medium articles and Twitter posts can also be time consuming and mentally exhausting as this process can involve a great deal of trial and error, demanding hours of debugging error messages full of machine learning jargon.

In summary, this research project aims to make the following contributions:

1. The application will have an interface for user to connect scenes (containing text and images) together to make the storyboard and assist them in outlining.
2. The application will also support writers in finetuning a diffusion model (assuming they have limited knowledge on generative AI). It will facilitate the process for designing characters, producing dataset(s) of character art and training the image generation model on the dataset(s) to produce more relevant images for constructing the storyboard. It will do so by optimising prompts for the diffusion model and facilitating the image generation process to reduce the mental burden on the writer (i.e. by providing advice and guidance at step of the process).
3. The image generation will also provide visual inspiration for character design.
4. The combination of plot ideas (from LLM) and more relevant image generation will collectively help address writer’s block.
5. The project will explore limitations of diffusion models and how these can be circumvented using methods for customising diffusion models. The prototype evaluation will demonstrate the extent to which these customisation methods work for this specific use case of assisting writers with storyboarding and detail how code provided by past researchers was modified to enhance the results from finetuned models.

The following section will explore relevant literature related to prompt optimisation tools, using image generation for story visualisation, writer’s block, using LLMs for creative writing and finetuning diffusion models. Section III will, then, describe how the scope of this research project was refined through the thematic analysis of initial user interviews and the resulting user requirements for the application. Section IV further explores the application design and justifications for utilising the chosen LLM and image generation models. Finally, Section V. will conclude with an overview of future work for this project including building, testing, and evaluating the prototype and the updated project timeline.

## II. Literature Review

**Note: Update this paragraph.**

This literature review section explains XYZ in section II-A,  XYZ in section II-B….summarize the structure of literature review here.

### **A. Writer’s block and creative needs**

Writer’s block occurs when a skilled writer is unable to generate fresh work for a set amount of time [10]. To determine how to design an application that will address writer’s block, it is necessary to explore some literature that discusses this phenomenon in relation to generative AI. Namely, Gero et al. conducted interviews with creative authors and shared their findings on how AI writing tools can best support them [11]. Gero et al. note that it would be helpful to manage the writer’s expectations regarding the AI system’s abilities by providing guidance in the form of context, examples of how the AI system will perform for various tasks, and the interface design which separates AI generated text from story inputted by the user [11].

When designing an application for writers, it is also important to recognise the types of support that would be helpful. This research project will target the planning stage of writing which highly conceptual, involving establishing objectives and developing ideas [11]. For planning, Gero et al. record that writers expressed the desire for inspiration and assistance with writer’s block – particularly when they had difficulty making narrative decisions, they would like to choose from multiple scenarios and/or adapt these computer-generated ideas [11]. An earlier paper also highlights that writers require assistance with establishing plot, ensuring narrative consistency, and creating compelling story arcs [12].

When considering how to design the storyboard functionality for the application in this research project, Kreminski and Martens’ discussion of models for plotting including the Hero’s Journey and Freytag pyramid are useful to consider to design the template [12].

### **B. AI Storyboarding software**

### **C. Prompt recommendation tools**

A crucial first step in using generative image models, such as diffusion models, is providing effective prompts. According to Jiang et. al [13], some challenges of prompt engineering include finding examples for prompts, troubleshooting and assessing if prompts are improving. To enhance prompt engineering for non-technical users, research has investigated user-friendly approaches for crafting effective prompts for generative image models. For example, Promptify provides prompt recommendations for text to image models using the LLM GPT-3.5 to suggest keywords and facilitating the organisation of generated images [14]. In contrast, PromptMagician utilises the DiffusionDB dataset’s collection of prompts and images to produce keyword recommendations and filters resulting images [6].

This project addresses challenges that users, who are unfamiliar with machine learning models, encounter when attempting to articulate their preferences and requirements to generative AI using prompts.

This project will build upon prior research by implementing the recommendations provided in these papers, including using advanced models such as GPT-4 for prompting, adopting chain of thought prompting for prompt clarification, introducing negative prompts, and the enhancement of user control over multiple hyper-parameters [6], [14].

### **D. Sequential image generation for illustrating stories**

Previous research has investigated the use of generative image models to illustrate fairy tales. For instance, Ruskov discusses using Midjourney v4 to create illustrations for fairytales and devising an prompt refinement process for image creation [15]. Another example is FairyTailor, a platform for blending generated text with images from Unsplash to construct children’s fairytales and offers features such as auto-completion [16].

### **E. LLMs for creative writing**

Two papers that explore using LLMs to augment the writing process, [17] and [2], develop applications with writing interfaces that integrate writing suggestions generated by AI.

Multiple papers have discussed writing tools powered by AI and how these can automate parts of the writing process. For example, Dramatron can produce scripts and screenplays by using prompt chaining to create structural context [3]. The method explored in this paper [3], termed ‘hierarchical story generation’, can provide inspiration for working with an LLM during this research project. The paper describes how a log line, which condenses the narrative into a few sentences with information about the setting, protagonist, and conflict, is used to generate the story [3]. In order to build the plot ideas generation feature, it will be necessary to use few shot learning and chain of thought prompting to make OpenAI API calls. This research project will also use one sentence plot summaries as narrative context to guide plot idea generation from the LLM. While the precise prompts from the Dramatron paper [3] will not be applicable, understanding how they generated a cohesive script through prompt programming will be valuable for determining how to provide more relevant plot suggestions to users of the application developed during this research project. However, one of the limitations that was flagged in [3] is that LLMs’ context window is restricted to 2048 tokens in advanced models and this is no longer applicable. OpenAI’s GPT-4-turbo models, for instance, have context windows of up to 128,000 tokens [18] and as this context window continues to increase in the future – it will enable LLMs to produce more accurate plot suggestions based on larger amounts of information about the story.

Another program, End-to-End Story Plot Generator, can produce a plot using LLaMA2 which removes the rate limit associated with OpenAI API calls and allows for large-scale text generation [4]. Since this paper [4] involves finetuning multiple LLMs and building datasets, for training the models, to improve the quality of the plots generated – it wouldn’t be feasible to implement their solution precisely. However, the information in this paper [4] about designing and structuring effective prompts to generate details about premise and characters using an LLM is useful. For example, they note that LLaMA2 emphasised the character’s appearance, so they modified the prompt to ensure that the LLM’s output contained information about the character’s profession and connections to other characters [4].

Other research has explored enabling writers to influence story outline generation by drawing lines that reflect the protagonist’s fate [19].

Singh *et al.* designed an interface that also provides images from Unsplash and audio from Freesound to further enhance the writing process [17]. Conversely, Yuan *et al*. incorporate a chatbot which writers can use for generating ideas or conducting research [2].  Furthermore, there are also publicly available AI writing tools for use cases such as copywriting and marketing [20]. To that end, a fiction writing tool called Sudowrite uses Transformer models such as GPT-3 by OpenAI and the diffusion model DALL·E to generate images, text autocompletions, writing feedback, and plot ideas [21], [22].

Wordcraft employed a Language Model (LLM) in conjunction with an editor and an interface that prompted users to extend the narrative, propose rewrites, and request additional details [2]. Wordcraft was used for tasks such as generating ideas, rewriting scenes and copyediting [2].

### **F. Personalising diffusion models**

Models such as stable diffusion (cite) can be further customised using techniques such as Dreambooth,  LoRA, Custom Diffusion, and Textual Inversion (cite all four!).

[Dreambooth](https://huggingface.co/docs/diffusers/v0.11.0/en/training/dreambooth) enables personalisation of stable diffusion using up to 5 images of the subject. Custom Diffusion, unlike Dreambooth, enables personalising the model for multiple subjects which would be more ideal for my application. The [code for Custom Diffusion is on Github](https://github.com/adobe-research/custom-diffusion) and [there’s a model training guide on Hugging Face](https://huggingface.co/docs/diffusers/training/custom_diffusion) with instructions for modifying the script.

- Describe how they can be personalised – i.e. finetuning certain layers.
- Talk about the limitations of the techniques. (check MPS paper)
- Explain how my research aims to develop a better understanding of these limitations and perhaps find ways around them?? **Note: Is this OK?**

## III. Refining scope of project

### Formative Interviews

Purpose: Generating user requirement with semi-formal user survey

- Survey design: questions that were asked and structure of the interview
- Criteria for user requirement how semi-formal user interviews were conducted (via Zoom)
- Quotes from interview transcription and thematic analysis results

### Initial user requirements and site map (too many features, not focused on a research direction)

### Test out using Sudowrite, test out using GPT and Dall E for writing tasks

### Updated user requirements

## IV. Design

### *A. Selecting the LLM and Text to Image Generation Models*

**Note: Describe different ways to approach this problem**

**Note: Cite the website for the APIs and models**

Some of the available LLM and text to image generative models APIs include: OpenAI’s APIs for gpt-4 and dall 3, PaLM2 and Imagen from Google, Claude by Anthrophic, Midjourney, Stable Diffusion and Gemini by Google. Out of these, OpenAI’s APIs were selected because these will be easier to integrate and cost effective. In addition, gpt-4 and Dall E 3 are relatively new so less research has been conducted on their functionalities and limitations. Since both gpt-4 and Dall E have been developed by the same company, they will also work better together when gpt-4 is used to design prompts for image generation. Stable Diffusion will also be used because it is free, open source code is available which means that the model can be finetuned, there are no restrictions for image generation or content policy.

Finetuning the model is the most effective and reliable strategy for producing consistent character art. Since there are few dedicated applications to helping new users navigate the process, this project will aim to build an application like that and also demonstrate how a user could streamline their writing process with it.

### *A. Choosing an approach for addressing image generation model limitations*

From the research I have read, it is not possible to fix this problem using prompt engineering with a pretrained LLM unless you further finetune that LLM using techniques such as reinforcement learning and a large dataset containing prompts (cite 2 papers on this). Since this will be difficult to achieve in the time frame of the project, it is more feasible to focus on alternative approaches such as finetuning with Dreambooth and Custom Diffusion and using prompt engineering with an LLM to design better prompts for an text to image generation model.

Here is the proposed solution:

1. Finetuning the model with 2-3 subjects using custom diffusion.

I intend to follow the process described in the research paper for Custom Diffusion and also Dreambooth – i.e. put together the datasets, run and modify code for my specific use case and determine how to improve the output by adjusting parameters. There have been some articles on this but there is no single comprehensive guide and since I would have to make changes to the code to achieve my aim, this could be a research contribution.

I also noticed that the prompts used to test out these finetuning methods in the research papers are simplistic – along the lines of “V* cat sitting on chair” and my assumption is that the pretrained model is quite familiar with images of cats, so it is not as difficult for it to generate images of the same cat in multiple settings. However, I want the user to be able to depict more complex scenes from their story to visualise them better so this would involve using prompts that are at least 10 or more words long. Therefore, I think it would be interesting to see how these methods fare when combined with more complicated prompts and whether the longer prompt can be optimised in some way using GPT-4 to get better results.

- The objective of this research project is to build an image generation application that will assist writers in brainstorming ideas by providing visual inspiration for worldbuilding and character design.
- Currently, the system will be designed to offer writing prompts and plot ideas for brainstorming, along with a storyboarding feature that generates images to illustrate story scenes. Users will also have the ability to access previous prompts and images.
- This application will be built using Python Flask for the backend and an JavaScript framework (i.e. React) for the frontend. It will leverage an LLM (i.e. GPT-4.0) for prompt refinement and creative writing assistance in conjunction with a diffusion model (i.e. Stable Diffusion or DALL·E 3) for image generation. Before selecting a model, the costs for the API and the advantages of each model will be compared. However, techniques such as Dreambooth will be useful for finetuning Stable Diffusion [23] in particular to ensure the uniform portrayal of characters across multiple images [15]. Prompt engineering for the diffusion model will employ few-shot and zero-shot prompting techniques, drawing examples from the internet and initial model output, as recommended in this paper [13].
- The choice of these models and technologies is informed by prior applications with similar approaches. **Note: Is additional justification needed here?**

**Interface design**

- The user interface design will be completed in Figma.

## 

Here are the components for this application and how I want to structure the user interaction:

1. The user will enter a one-line summary of their short story or novel. (This can be used later to provide context to GPT-4 when generating plot ideas for the storyboard.)
2. The user will decide who the main cast of 3-5 characters are. They will search for images and/or generate them if they can’t find suitable images.
3. I can use Pinterest and Unsplash APIs to display image search results to the user so that they don’t have to leave the application. They can select 1 or more of these images.
4. Or they can generate a brand new image by entering a prompt. Prompt recommendations will be provided by GPT-4 with keyword and style suggestions (and I will use few-shot prompting to instruct GPT for implementing this). For this part, it would be useful for the user to request GPT to help them modify the prompt to emphasize or correct aspects of the image. This has had variable results when I attempted it myself, but the overall result was that the images improved. The user can adjust the prompts until the images they generate are suitable and then save these.
5. Note – to finetune the model, the dataset needs to have around 3-5 images of the subject, so it is necessary to use prompt engineering to generate similar images of a single subject (or to find multiple images of a person via image search APIs).
6. The user will open the storyboard section of the application which will have boxes where they can enter text or upload/insert images. The user can, then, select up to 3 characters (custom diffusion cannot handle more than three subjects) that they want to appear across multiple scenes and the stable diffusion model will be tuned on these images. This could take up to 10 minutes (this is just an estimate**)**.
7. While the model is being tuned, the user can plan out the narrative arc for their story. The template for the storyboard can follow the traditional three-act structure – inciting incident, rising action, and resolution. This will be framed for the writer as a series of suggestions to guide them.
8. The user can enter some information for each act and/or scene of their story.
9. The user can also click a button to generate plot ideas for the next act or scene. This will send instructions to GPT-4 to generate a few ideas based on some context about the story (i.e. the short summary the user entered earlier and any information that they have entered into the storyboard will be sent via prompt chaining). The context length for GPT-4 has increased so it will be possible to send much more information. Additionally, some of the information can also be summarized before it is sent via the API to reduce the number of tokens.
10. The user will be notified that the fine-tuned stable diffusion model is ready for use, and they can copy paste information about the scenes they want images for into the prompt field of the image generation model. The user-entered prompt will be enhanced using GPT-4. (Note: There will have to be some guidance to the user regarding using the modifier token that represents the subject in the prompt.)
11. The user can generate up to 3 images for each scene and select the one they prefer to add to the storyboard.
12. The user can also move the scenes (a box containing some text and image(s)) around and rearrange them until they are satisfied with the narrative progression.

My goal is also to develop an application with an interface that will make this personalised model more accessible for a user with less technical expertise and familiarity with tuning an image generation model but wants to generate art for their story.

Storyboard template will be the hero’s journey **– need to find an article on this to cite**

## Future work

**Prototype evaluation and user testing**

- The prototype evaluation will focus on writers with limited generative AI experience, and involve both remote and in-person testing. Data will be collected within the application (i.e. logging user interactions in a database) to enable remote testing. The study aims to assess system usability, especially for brainstorming, storyboarding, and plot generation.
- To further evaluate the prototype, criteria will be established for assessing image quality and distinctiveness. User feedback and comparisons with images generated by other models can contribute to improving the model.

For the research contribution, I would like to evaluate different methods for finetuning stable diffusion on a small dataset of images of the target subject using techniques such as Dreambooth and Custom Diffusion.

**Learning tools & technologies**

- Front end framework + back end framework (Flask) integrated with a diffusion model
- OpenAI APIs documentation
- Finetuning diffusion model via google colab and hosting on hugging face
- Prompt engineering

**Updated timeline**

Due to a lack of familiar with the field of machine learning, the first semester involved building an understanding of working with LLMs and diffusion models, prompt engineering, and training or finetuning models through courses on Deep Learning AI (cite).

The tasks and corresponding deadlines for this project are depicted in Figure 1 below. The dissertation and research paper, respectively, will be submitted one week in advance of the official deadlines, allowing for flexibility in the schedule.

Although the task breakdown appears comprehensive, the hard deadlines are specified for each milestone (i.e. Literature Review) in GitHub Projects and the soft deadlines for tasks associated with these milestones can be adjusted as necessary.

**Add the following image:** Fig. 1. Task list in GitHub Projects with deadlines and phases

## Conclusion

- Do I need to mention ethical implications – i.e. can be used for deception, can negatively impact artists by using copyrighted works
- Essentially, this tool will streamline the brainstorming portion of the writing process by providing reference images for worldbuilding and character description and assisting in refining ideas and developing conflict. This application will also leverage an LLM to supply personalised plot ideas.

**Limitations**

- Do I need this?

**Areas for future work**

- Content moderation for inputs and outputs will make this application production ready
- May be able to get better personalised prompting suggestions by finetuning an LLM such as Llama
- Exploring other finetuning methods for SDXL and finding ways to speed up this process so that the user doesn’t have to wait too long to use the finetuned model
- Not using image generation model APIs because these can become obsolete as new APIs are released and over-reliance on them will make application maintenance long term difficult => better to switch to open source models that can be hosted on servers

## References

[1]  K. Kenthapadi, H. Lakkaraju, and N. Rajani, ‘Generative AI meets Responsible AI: Practical Challenges and Opportunities’, in *Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, in KDD ’23. New York, NY, USA: Association for Computing Machinery, Aug. 2023, pp. 5805–5806. doi: 10.1145/3580305.3599557.

[2]  A. Yuan, A. Coenen, E. Reif, and D. Ippolito, ‘Wordcraft: Story Writing With Large Language Models’, in *27th International Conference on Intelligent User Interfaces*, in IUI ’22. New York, NY, USA: Association for Computing Machinery, Mar. 2022, pp. 841–852. doi: 10.1145/3490099.3511105.

[3]  P. Mirowski, K. W. Mathewson, J. Pittman, and R. Evans, ‘Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals’, in *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*, in CHI ’23. New York, NY, USA: Association for Computing Machinery, Apr. 2023, pp. 1–34. doi: 10.1145/3544548.3581225.

[4]  H. Zhu *et al.*, ‘End-to-end Story Plot Generator’. arXiv, Oct. 12, 2023. doi: 10.48550/arXiv.2310.08796.

[5]  ‘Generative Models by Stability AI’. Stability AI, Oct. 31, 2023. Accessed: Nov. 01, 2023. [Online]. Available: https://github.com/Stability-AI/generative-models

[6]  Y. Feng *et al.*, ‘PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation’. arXiv, Aug. 15, 2023. doi: 10.48550/arXiv.2307.09036.

[7]  ‘Large-scale Text-to-Image Generation Models for Visual Artists’ Creative Works | Proceedings of the 28th International Conference on Intelligent User Interfaces’. Accessed: Jan. 29, 2024. [Online]. Available: https://dl.acm.org/doi/abs/10.1145/3581641.3584078

[8]  V. Liu, H. Qiao, and L. Chilton, ‘Opal: Multimodal Image Generation for News Illustration’, in *Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology*, in UIST ’22. New York, NY, USA: Association for Computing Machinery, Oct. 2022, pp. 1–17. doi: 10.1145/3526113.3545621.

[9]  P. Chambon, C. Bluethgen, C. P. Langlotz, and A. Chaudhari, ‘Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains’. arXiv, Oct. 08, 2022. doi: 10.48550/arXiv.2210.04133.

[10]  I. Gilburt, ‘A machine in the loop: the peculiar intervention of artificial intelligence in writer’s block’, *New Writ.*, vol. 0, no. 0, pp. 1–12, 2023, doi: 10.1080/14790726.2023.2223176.

[11]  K. I. Gero, T. Long, and L. B. Chilton, ‘Social Dynamics of AI Support in Creative Writing’, in *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*, in CHI ’23. New York, NY, USA: Association for Computing Machinery, Apr. 2023, pp. 1–15. doi: 10.1145/3544548.3580782.

[12]  M. Kreminski and C. Martens, ‘Unmet Creativity Support Needs in Computationally Supported Creative Writing’, in *Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022)*, T.-H. ‘Kenneth’ Huang, V. Raheja, D. Kang, J. J. Y. Chung, D. Gissin, M. Lee, and K. I. Gero, Eds., Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 74–82. doi: 10.18653/v1/2022.in2writing-1.11.

[13]  E. Jiang *et al.*, ‘PromptMaker: Prompt-based Prototyping with Large Language Models’, in *Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems*, in CHI EA ’22. New York, NY, USA: Association for Computing Machinery, Apr. 2022, pp. 1–8. doi: 10.1145/3491101.3503564.

[14]  S. Brade, B. Wang, M. Sousa, S. Oore, and T. Grossman, ‘Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models’. arXiv, Apr. 18, 2023. doi: 10.48550/arXiv.2304.09337.

[15]  M. Ruskov, ‘Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales’. arXiv, Aug. 25, 2023. doi: 10.48550/arXiv.2302.08961.

[16]  E. Bensaid, M. Martino, B. Hoover, and H. Strobelt, ‘FairyTailor: A Multimodal Generative Framework for Storytelling’. arXiv, Jul. 12, 2021. doi: 10.48550/arXiv.2108.04324.

[17]  N. Singh, G. Bernal, D. Savchenko, and E. L. Glassman, ‘Where to Hide a Stolen Elephant: Leaps in Creative Writing with Multimodal Machine Intelligence’, *ACM Trans. Comput.-Hum. Interact.*, vol. 30, no. 5, p. 68:1-68:57, Sep. 2023, doi: 10.1145/3511599.

[18]  ‘OpenAI Platform’, OpenAI API Models. Accessed: Jan. 30, 2024. [Online]. Available: https://platform.openai.com/docs/models/overview

[19]  ‘TaleBrush: Sketching Stories with Generative Pretrained Language Models | Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems’. Accessed: Jan. 29, 2024. [Online]. Available: https://dl.acm.org/doi/10.1145/3491102.3501819

[20]  H. Guinness, ‘The best AI writing generators in 2023’, Zapier. Accessed: Oct. 31, 2023. [Online]. Available: https://zapier.com/blog/best-ai-writing-generator/

[21]  ‘Sudowrite’, Sudowrite. Accessed: Oct. 31, 2023. [Online]. Available: https://www.sudowrite.com/

[22]  ‘New beta feature: Canvas’, Sudowrite. Accessed: Nov. 01, 2023. [Online]. Available: https:///changelog/16456, https://feedback.sudowrite.com/changelog/16456

[23]  Patil, Pedro Cuenca, and Valentine Kozin, ‘Training Stable Diffusion with Dreambooth using Diffusers’, Hugging Face. Accessed: Nov. 01, 2023. [Online]. Available: https://huggingface.co/blog/dreambooth
