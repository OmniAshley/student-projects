{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame()\n",
    "for i in range(1,27):\n",
    "    filename = f\"rawdata/{i}.json\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            df = pd.json_normalize(data.get('data'))\n",
    "\n",
    "    # Add duration by using next time - this time\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['duration'] = df['time'].diff().apply(lambda x: x.total_seconds())\n",
    "    df['duration'] = df['duration'].shift(periods=-1)\n",
    "    df.at[df.index[-1], 'duration'] = random.randint(5,20)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    df = df[['url','type', 'duration']]\n",
    "    df = df.reset_index()\n",
    "    dfs = pd.concat([dfs,df],axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "punc=string.punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "dfs['re_url'] = dfs['url'].replace(regex={r'http://': '', 'https://': '', 'www.': '',\n",
    "                                          '.edu':'', '.org':'', '.net':'',\n",
    "                                          '.uk':'', 'chrome':'', 'sourceid':'', 'utf':'',\n",
    "                                          'and':'', '.cn':''})\n",
    "# print(dfs['re_url'][1])\n",
    "\n",
    "for i in range(len(dfs['re_url'])):\n",
    "    # Remove query parameters and special characters\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'\\.com\\b', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'\\.ac\\b', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'[\\.\\?\\!\\/\\&\\+\\=\\-\\%\\#]', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'\\bin\\b', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'\\bthe\\b', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'\\bfor\\b', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'html', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'php', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'\\be5\\b', ' ', dfs.loc[i,'re_url'])\n",
    "\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'[A-Z]', lambda m: m.group(0).lower(), dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'(?<![a-z])[a-z](?![a-z])', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'\\b\\d+\\b', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'\\b\\w\\d+\\b', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'\\b\\w{1,2}\\b', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'\\butf\\b', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'[^\\w\\s]', ' ', dfs.loc[i,'re_url'])\n",
    "    dfs.loc[i,'re_url'] = re.sub(r'\\b\\w{1,2}\\d+\\b', ' ', dfs.loc[i,'re_url'])\n",
    "\n",
    "dfs['re_url'].apply(lambda x: [word for word in x if word not in punc])\n",
    "dfs['re_url'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "dfs['tok_url'] = dfs['re_url'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "from nltk import pos_tag\n",
    "dfs['pos_tags'] = dfs['tok_url'].apply(pos_tag)\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "dfs['wordnet_pos'] = dfs['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "dfs['lemma'] = dfs['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the url vocabulary\n",
    "from collections import Counter\n",
    "vocabulary = []\n",
    "for row in dfs['lemma']:\n",
    "    vocabulary.extend(row)\n",
    "\n",
    "vocabulary = [word.translate(str.maketrans(\"\", \"\", string.punctuation)) for word in vocabulary]\n",
    "vocabulary = [re.sub(r'\\b\\w{1,2}\\b', '', word) for word in vocabulary]\n",
    "vocabulary = [re.sub(r'\\b\\w+\\d+', '', word) for word in vocabulary]\n",
    "vocabulary = [re.sub(r'\\b\\d+\\w+', '', word) for word in vocabulary]\n",
    "vocabulary = list(filter(None, vocabulary))\n",
    "\n",
    "freq_dist = Counter(vocabulary)\n",
    "sorted_vocab = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
    "vocab_size = 800  \n",
    "url_vocab = [token for token, freq in sorted_vocab[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "le = LabelEncoder()\n",
    "dfs['type'] = le.fit_transform(dfs['type'])\n",
    "\n",
    "# Encoding urls using the vocabulary and convert all the type into float32\n",
    "word_to_label = {word: i for i, word in enumerate(url_vocab)}\n",
    "dfs['encoded_urls'] = dfs['lemma'].apply(lambda x: [word_to_label[word] for word in x if word in url_vocab])\n",
    "\n",
    "dfs_encoded = dfs[['index','type', 'duration', 'encoded_urls']]\n",
    "dfs_encoded['type'] = dfs_encoded['type'].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "dfs_encoded = dfs_encoded[['index','type', 'duration']]\n",
    "\n",
    "padded_urls = pad_sequences([x for x in dfs['encoded_urls']], maxlen=22, padding='post')\n",
    "# padded_urls = list(padded_urls.astype('float32'))\n",
    "df2 = pd.DataFrame(padded_urls, columns= [f'{i}' for i in range(1, 23)])\n",
    "dfs_encoded = pd.concat([dfs_encoded, df2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into time sequences, 4 events forms a time step, and put them into input and output lists\n",
    "samples = []\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "start_index = 0\n",
    "for row_index in range(len(dfs_encoded)-1):\n",
    "        if (dfs_encoded.iloc[row_index+1]['index'] == 0 or row_index+1 == (len(dfs_encoded)-1)):\n",
    "                samples = dfs_encoded.iloc[start_index:row_index+2, 1:].values.tolist()\n",
    "                for i in range(4, len(samples)):\n",
    "                        time_step = samples[i-4:i]\n",
    "                        X_sequences.append(time_step)\n",
    "                        y_sequences.append(samples[i][2:])\n",
    "                        i+=1\n",
    "                start_index = row_index+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_sequences,y_sequences,test_size=0.2,random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, SimpleRNN\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "# model.save('my_model.h5')\n",
    "# model = keras.models.load_model(model, custom_objects={'binary_activation': binary_activation})\n",
    "model.add(LSTM(32, return_sequences=True, input_shape=(4,24)))\n",
    "# model.add(LSTM(32, return_sequences=True, input_shape=(5,658)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32, input_shape=(4,24)))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(22, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=10, epochs=150, validation_data=(X_test, y_test))\n",
    "\n",
    "# train the model\n",
    "# model.fit(X_train, y_train, epochs=100, batch_size=10)\n",
    "\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(\"Test set loss: {:.3f}\".format(loss))\n",
    "# print(\"Test set accuracy: {:.3f}\".format(accuracy))\n",
    "\n",
    "# predict on test data\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # convert the probability distribution to the sequence of numbers\n",
    "# y_pred_seq = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "print(\"Training.....\")\n",
    "model.save('modelX.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "styleGan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
